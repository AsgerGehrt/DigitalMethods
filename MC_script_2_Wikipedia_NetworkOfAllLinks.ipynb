{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 2: Make two different networks based on all links found on a wikipedia page \n",
    "\n",
    "In the script \"MCTutorial2_Wikipedia_InText_reference_Network_final\" we looked at all the links found in the main text of a Wikipedia article. By doing so, we exclude links that has been assigned to the article based on a template. As wikipedia puts it: _\"Templates are pages that are embedded (transcluded) into other pages to allow for the repetition of information\"_ ([Wikipedia templates](https://en.wikipedia.org/wiki/Wikipedia:Templates)). The template can be found in the buttom of every Wikipedia page: \n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549631130/Mapping%20controversies%202019/Template.jpg\" title=\"Category:circumcision\" style=\"width: 700px;\" /> \n",
    "\n",
    "In this tutorial, we will include all \"internal\" links to other Wikipedia pages found on a page (i.e. the links found in the templates and in the main text). \n",
    "\n",
    "This script takes as input a file with category members from Wikipedia (e.g. \"cat_members_circumcision_depth_2.json\") and builds two networks. One network with the cat members (only) connected by the links and one with the cat-mebers + all the pages they point to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries\n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.\n",
    "\n",
    "\n",
    "__Obs: in this workbook we will be using the wikipedia and networkx libraries. If you have already installed them once, there is no need to do it again. You may simply skip to step 2.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipediaapi library has been imported\n",
      "NetworkX library has been imported\n"
     ]
    }
   ],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed \n",
    "\n",
    "import sys\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import wikipediaapi\n",
    "    print(\"wikipediaapi library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"wikipediaapi library not found. Installing...\")\n",
    "    !pip install wikipedia-api\n",
    "    try:#... and try to import it again\n",
    "        import wikipediaapi\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the wikipediaapi library. Please check your internet connection and consult output from the installation below\")\n",
    "try:\n",
    "    import networkx\n",
    "    print(\"NetworkX library has been imported\")\n",
    "except:\n",
    "    print(\"NetworkX library not found. Installing...\")\n",
    "    !pip install networkx\n",
    "    \n",
    "    try:\n",
    "        import networkx\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the NetworkX library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make the networks of all links\n",
    "\n",
    "The next step is to make the network. Here, you need to input the path to the json files you got from the MCTutorial1_Wikipedia_HarvestCatMembers_final script. \n",
    "\n",
    "If the JSON files are in the same directory as the scripts, you only need to input relational directions (i.e. the name of the json file e.g. cat_members_circumcision_depth_2)\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549444568/Mapping%20controversies%202019/Script_json_same_folder_in_text.jpg\" title=\"Folder\" style=\"width: 800px;\" /> \n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "cat_members_all=[]\n",
    "print(\"Enter the name of the category members json file you wish to use for keyword search (e.g.cat_members_circumcision_depth_2). If you have multiple files separate them with a comma\")\n",
    "filename= input()\n",
    "if \",\" in filename:\n",
    "\n",
    "    for each in filename.split(\",\"):\n",
    "\n",
    "\n",
    "        if not each.endswith(\".json\"):\n",
    "            path=each+\".json\"\n",
    "        else: \n",
    "            path=each\n",
    "            each=each.split(\".\")[0]\n",
    "        with open(path) as jsonfile:\n",
    "            cat_members = json.load(jsonfile)\n",
    "            jsonfile.close()\n",
    "        for every in cat_members:\n",
    "            cat_members_all.append(every)\n",
    "else:\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "    if not filename.endswith(\".json\"):\n",
    "        path=filename+\".json\"\n",
    "    else: \n",
    "        path=filename\n",
    "        filename=filename.split(\".\")[0]\n",
    "    with open(path) as jsonfile:\n",
    "        cat_members_all = json.load(jsonfile)\n",
    "        jsonfile.close()\n",
    "\n",
    "    \n",
    "    \n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "print(\" \")\n",
    "wiki_wiki = wikipediaapi.Wikipedia(lan)\n",
    "\n",
    "\n",
    "seen = []\n",
    "network = {}\n",
    "print(\"Harvesting all links from \"+str(len(cat_members_all))+\" wikipedia pages. This might take a while...\")\n",
    "print(\"\")\n",
    "\n",
    "count=1\n",
    "for each in cat_members_all:\n",
    "    title=each[\"title\"]\n",
    "    if count % 50 == 0:\n",
    "        print(\"All links harvested from \"+str(count)+\" pages out of \"+str(len(cat_members_all))+\". Continuing harvest...\")\n",
    "    if not title in seen:\n",
    "        seen.append(title)\n",
    "        try:\n",
    "        \n",
    "            page=wiki_wiki.page(title)\n",
    "            text_links = []\n",
    "            links = page.links\n",
    "            for link_title in sorted(links.keys()):\n",
    "                text_links.append(link_title)\n",
    "            network.update({title:text_links})\n",
    "\n",
    "        except:\n",
    "            print('SKIPPED: '+title)\n",
    "            print(\"\")\n",
    "    count=count+1\n",
    "    \n",
    "print(\"All pages harvested...\")\n",
    "new_cat_members={}\n",
    "for each in cat_members_all:\n",
    "    new_cat_members[each[\"title\"]]={\"level\":each[\"level\"]}\n",
    "    \n",
    "membersonly_edges = []\n",
    "all_edges = []\n",
    "members = network.keys()\n",
    "print(\"Calculating networks...\")\n",
    "print(\"\")\n",
    "for source in network:\n",
    "    for target in network[source]:\n",
    "        edge = (source,target)\n",
    "        all_edges.append(edge)\n",
    "        if target in members:\n",
    "            membersonly_edges.append(edge)\n",
    "print(\"Saving networks...\")\n",
    "print(\"\")\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(membersonly_edges)\n",
    "nx.write_gexf(G,'MCTutorial2_2_'+ filename+'_AllLinksNet_membersonly.gexf')\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(all_edges)\n",
    "for each in G.nodes:\n",
    "    if each in members:\n",
    "        G.nodes[each]['member_level'] = 'Level '+str(new_cat_members[each][\"level\"])\n",
    "    else:\n",
    "        G.nodes[each]['member_level'] = 'Not a member'\n",
    "nx.write_gexf(G, 'MCTutorial2_2_'+filename+'_AllLinksNet_allpages.gexf')\n",
    "print(\"The script is done. You can find your network files by following these paths: \")\n",
    "print(\"\")\n",
    "locale=!pwd\n",
    "print(locale[0]+\"/\"+'MCTutorial2_2_'+filename+'_AllLinksNet_membersonly.gexf')\n",
    "print(\"\")\n",
    "print(locale[0]+\"/\"+'MCTutorial2_2_'+filename+'_AllLinksNet_allpages.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
