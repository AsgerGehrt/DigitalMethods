{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 6: Extract text from Wikipedia articles \n",
    "\n",
    "This script will extract all text from the pages you input. The text will be outputted as a .csv file, which works with both spreadsheet editors and the tool Cortext we will user later in the course. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries\n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.\n",
    "\n",
    "\n",
    "__Obs: in this workbook we will be using the wikipediaapi library. If you have already installed it once, there is no need to do it again. You may simply skip to step 2.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import wikipediaapi\n",
    "    print(\"Wikipedia api library has been imported\")\n",
    "except:\n",
    "    print(\"wikipedia api library not found. Installing...\")\n",
    "    !pip install wikipedia-api\n",
    "    \n",
    "    try:\n",
    "        import wikipediaapi\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the wikipedia api library. Please check your internet connection and consult output from the installation below\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2A: Harvest the text from a single page and/or pages from a category members json file\n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):\n",
      "en\n",
      "Please enter a prefix for the output file: \n",
      "wiki_corona_EU\n",
      "Collecting text from 21 pages...\n",
      "CSV file saved. You can find the network by following this path: \n",
      "/c/Users/ago/Documents/Jupyter/Mapping controversies 2020/wiki_corona_EU_TextFromArticles.csv\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import csv\n",
    "import json\n",
    "\n",
    "pages=[\"2020 coronavirus pandemic in Afghanistan\",\"2020 coronavirus pandemic in Austria\",\"2020 coronavirus pandemic in Belgium\",\"2020 coronavirus pandemic in Cyprus\",\"2020 coronavirus pandemic in Denmark\",\"2020 coronavirus pandemic in the Netherlands\",\"2020 coronavirus pandemic in Finland\",\"2020 coronavirus pandemic in Germany\",\"2020 coronavirus pandemic in Greece\",\"2020 coronavirus pandemic in Hungary\",\"2020 coronavirus pandemic in Iran\",\"2020 coronavirus pandemic in Israel\",\"2020 coronavirus pandemic in Italy\",\"2020 coronavirus pandemic in Norway\",\"2020 coronavirus pandemic in Spain\",\"2020 coronavirus pandemic in Sweden\",\"2020 coronavirus pandemic in Switzerland\",\"2020 coronavirus pandemic in Turkey\",\"2020 coronavirus pandemic in the United Kingdom\",\"2020 coronavirus pandemic in Serbia\",\"2020 coronavirus pandemic in Bosnia and Herzegovina\"]\n",
    "\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language=lan,\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "print(\"Please enter a prefix for the output file: \" )\n",
    "filename=input()\n",
    "filename=filename+\"_TextFromArticles.csv\"\n",
    "\n",
    "print(\"Collecting text from \"+str(len(pages))+\" pages...\")\n",
    "csv_path=filename\n",
    "\n",
    "headers=[\"Page\", \"Page text\"]\n",
    "with open(csv_path,\"a\", encoding='utf-8', newline='\\n') as tsvfile:\n",
    "    wr = csv.writer(tsvfile, delimiter=';')   \n",
    "\n",
    "    wr.writerow(headers)\n",
    "\n",
    "for page in pages:\n",
    "    p_wiki = wiki_wiki.page(page)\n",
    "    page_text=p_wiki.text.lower()\n",
    "    csv_list=[page,page_text]\n",
    "\n",
    "    with open(csv_path,\"a\", encoding='utf-8', newline='\\n') as tsvfile:\n",
    "        wr = csv.writer(tsvfile, delimiter=';')   \n",
    "\n",
    "        wr.writerow(csv_list)\n",
    "\n",
    "\n",
    "print('CSV file saved. You can find the network by following this path: ')\n",
    "locale=!pwd\n",
    "print(locale[0]+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2B - Harvest text from multiple category members files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import csv\n",
    "import json\n",
    "\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language=lan,\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "cat_dict = {}\n",
    "\n",
    "print(\"Enter the name of the category members json files you wish to use for keyword search (e.g.category_members_circumcision_depth_2). Separate multiple files with comma\")\n",
    "filename= input()\n",
    "filename=filename.strip()\n",
    "\n",
    "for each in filename.split(\",\"):\n",
    "    if each:\n",
    "        each=each.strip()\n",
    "        if not each.endswith(\".json\"):\n",
    "            path=each+\".json\"\n",
    "        else: \n",
    "            path=each\n",
    "            each=each.split(\".\")[0]\n",
    "        for word in each.split(\"_\")[2:]:\n",
    "            if word==\"depth\":\n",
    "                #print(filename.split(\",\")[0].split(\"_\").index(each))\n",
    "\n",
    "                index_=each.split(\"_\").index(word)\n",
    "        cat=\"\"\n",
    "        for word in each.split(\"_\")[2:index_]:\n",
    "            cat=cat+\" \"+word\n",
    "        cat_name=cat.strip()\n",
    "        #print(cat_name)\n",
    "        #cat_name=each.split(\"_\")[2]\n",
    "        with open(path) as jsonfile:\n",
    "            cat_members = json.load(jsonfile)\n",
    "            jsonfile.close()\n",
    "        cat_dict[cat_name]=[]\n",
    "        for every in cat_members:\n",
    "            cat_dict[cat_name].append(every['title'])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "filename=\"TextFromWikiarticlesMultipleCategories.csv\"\n",
    "\n",
    "csv_path=filename\n",
    "\n",
    "page_dict={}\n",
    "pages=[]\n",
    "for cat in cat_dict:\n",
    "    for page in cat_dict[cat]:\n",
    "        pages.append(page)\n",
    "for page in pages:\n",
    "    page_dict[page]={\"cats\":[]}\n",
    "\n",
    "for cat in cat_dict:\n",
    "    for page in cat_dict[cat]:\n",
    "        page_dict[page][\"cats\"].append(cat)\n",
    "\n",
    "print(\"Collecting text from \"+str(len(pages))+\" pages...\")\n",
    "\n",
    "for page in page_dict:\n",
    "    try:\n",
    "        p_wiki = wiki_wiki.page(page)\n",
    "        page_text=p_wiki.text.lower()\n",
    "        page_dict[page][\"text\"]=page_text\n",
    "    except:\n",
    "        print('skipping '+page)\n",
    "\n",
    "csv_list=[]\n",
    "\n",
    "\n",
    "headers=[\"page\",\"text\",\"unique_to_cat\"]\n",
    "\n",
    "for cat in cat_dict:\n",
    "    headers.append(cat)\n",
    "    \n",
    "csv_list.append(headers)\n",
    "    \n",
    "for page in page_dict:\n",
    "    if \"text\" in page_dict[page]:\n",
    "        entry=[page, page_dict[page][\"text\"]]\n",
    "        if len(page_dict[page]['cats']) == 1:\n",
    "            entry.append(page_dict[page]['cats'][0])\n",
    "        else:\n",
    "            entry.append('none')\n",
    "        for each in headers[3:]: \n",
    "            if each in page_dict[page][\"cats\"]:\n",
    "                entry.append(\"yes\")\n",
    "            else:\n",
    "                entry.append(\"no\")\n",
    "        csv_list.append(entry)\n",
    "\n",
    "with open(csv_path,\"w\", encoding='utf-8', newline='\\n') as tsvfile:\n",
    "    wr = csv.writer(tsvfile, delimiter=';')   \n",
    "\n",
    "    wr.writerows(csv_list)\n",
    "\n",
    "\n",
    "print('CSV file saved. You can find the network by following this path: ')\n",
    "locale=!pwd\n",
    "print(locale[0]+\"/\"+filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
