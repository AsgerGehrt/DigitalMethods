{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies tutorial x: search reddit submissions and posts for keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries \n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.  \n",
    "\n",
    "In order to run the installation, click on the cell below and press \"Run\" in the menu. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed to carry out the harvest of data from Wikipedia\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import praw\n",
    "    print(\"praw library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"praw library not found. Installing...\")\n",
    "    !pip install praw\n",
    "    try:#... and try to import it again\n",
    "        import praw\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the praw library. Please check your internet connection and consult output from the installation below\")\n",
    "try:\n",
    "    import pandas\n",
    "    print(\"pandas api library has been imported\")\n",
    "except:\n",
    "    print(\"pandas api library not found. Installing...\")\n",
    "    !pip install pandas\n",
    "    \n",
    "    try:\n",
    "        import pandas\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the pandas api library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import psaw\n",
    "    print(\"psaw api library has been imported\")\n",
    "except:\n",
    "    print(\"psaw api library not found. Installing...\")\n",
    "    !pip install psaw\n",
    "    \n",
    "    try:\n",
    "        import psaw\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the psaw api library. Please check your internet connection and consult output from the installation below\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Reddit app\n",
    "\n",
    "The first step is to create an app for reddit. This is done in order to get access to the API. You can do so by following the first step of [this tutorial](http://www.storybench.org/how-to-scrape-reddit-with-python/). \n",
    "\n",
    "### When you have the _14-characters personal use script_ and _27-character secret key_  run the cell below and input the information.\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1550562228/Mapping%20controversies%202019/reddit_app.jpg\" title=\"Category:circumcision\" style=\"width: 900px;\" /> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUN THIS CELL FIRST!\n",
    "\n",
    "\n",
    "print(\"Enter the 27 character secret key from the app page: \")\n",
    "secret=input()\n",
    "\n",
    "print(\"Enter the 14 character personal use script key from the app page: \")\n",
    "pus=input()\n",
    "\n",
    "print(\"Enter your app-name :\")\n",
    "app_name=input()\n",
    "\n",
    "print(\"Enter your reddit user name: \")\n",
    "user_name=input()\n",
    "\n",
    "print(\"Enter your reddit password: \")\n",
    "pw=input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Harvest the data from Reddit\n",
    "\n",
    "Search for a keyword in submissions, and harvest the submission thread. Limit to subreddit or search all Reddit.\n",
    "\n",
    "The script will output two csv files. One with all submissions and comments and one where users, up and downvotes, likes etc are aggregated on submission level. \n",
    "\n",
    "## BE AWARE! This script may take several hours or even days if the limit is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a limit of max 2.000 submissions. It cannot be increased\n",
      "Do you wan't to lower it (y/n)?\n",
      "y\n",
      "Enter the new limit (below 2.000)\n",
      "300\n",
      "Do you wan't to limit your search to a specific subreddit (y/n)?\n",
      "n\n",
      "Set the start date: (Format: yyyy-mm-dd) \n",
      "2017-01-01\n",
      "What would you like to call the file?\n",
      "\n",
      "Input the keyword you would like to search for: \n",
      "home sharing\n",
      "Enter user names you want to blacklist. If you want to blacklist multiple users, use comma separation\n",
      "\n",
      "Data harvested from 50 submissions out of maximum 300. Continuing harvest...\n",
      "Data harvested from 100 submissions out of maximum 300. Continuing harvest...\n",
      "Data harvested from 150 submissions out of maximum 300. Continuing harvest...\n",
      "Data harvested from 200 submissions out of maximum 300. Continuing harvest...\n",
      "Data harvested from 250 submissions out of maximum 300. Continuing harvest...\n",
      "The script is done! We have harvested 300 submissions with 2484 comments in total.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import praw\n",
    "import psaw\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "reddit = praw.Reddit(client_id=pus, \\\n",
    "                     client_secret=secret, \\\n",
    "                     user_agent=app_name, \\\n",
    "                     username=user_name, \\\n",
    "                     password=pw)\n",
    "api=psaw.PushshiftAPI(reddit)\n",
    "\n",
    "print(\"There is a limit of max 2.000 submissions. It cannot be increased\")\n",
    "print(\"Do you wan't to lower it (y/n)?\")\n",
    "limit_c=input()\n",
    "max_response_cache=2000\n",
    "if limit_c.lower()==\"y\":\n",
    "    print(\"Enter the new limit (below 2.000)\")\n",
    "    max_response_cache=int(input())\n",
    "if max_response_cache>2000:\n",
    "    max_response_cache=2000\n",
    "\n",
    "print(\"Do you wan't to limit your search to a specific subreddit (y/n)?\")\n",
    "sub_filter=input()\n",
    "sub_reddit=\"\"\n",
    "if sub_filter.lower()==\"y\":\n",
    "    sub_reddit=input()\n",
    "\n",
    "print(\"Set the start date: (Format: yyyy-mm-dd) \")\n",
    "start_date=input()\n",
    "start_year=int(start_date.split(\"-\")[0])\n",
    "start_month=int(start_date.split(\"-\")[1])\n",
    "start_day=int(start_date.split(\"-\")[2])\n",
    "start_epoch=int(dt.datetime(start_year, start_month, start_day).timestamp())\n",
    "\n",
    "\n",
    "print(\"What would you like to call the file?\")\n",
    "input_filename=input()\n",
    "\n",
    "print(\"Input the keyword you would like to search for: \")\n",
    "keyword=input()\n",
    "csv_headers=[\"Type\",\"id\",\"author\",\"body\",\"created\",\"up_votes\",\"down_votes\",\"likes\",\"depth\", \"parent_id\", \"url\", \"reports\", \"subreddit\", \"submission\"]\n",
    "blacklisted_users=[]\n",
    "print(\"Enter user names you want to blacklist. If you want to blacklist multiple users, use comma separation\")\n",
    "raw_blacklist=input()\n",
    "\n",
    "if \",\" in raw_blacklist:\n",
    "    for each in raw_blacklist.split(\",\"):\n",
    "        blacklisted_users.append(each.strip().lower())\n",
    "else: \n",
    "    blacklisted_users.append(raw_blacklist.strip().lower())\n",
    "if \".csv\" in input_filename:\n",
    "    filename=\"keyword_search_submissions_\"+input_filename\n",
    "    filename_2=\"keyword_search_aggregate_users_submissions_\"+input_filename\n",
    "else:\n",
    "    filename=\"keyword_search_submissions_\"+input_filename+\".csv\"\n",
    "    filename_2=\"keyword_search_aggregate_users_submissions_\"+input_filename+\".csv\"\n",
    "\n",
    "with open(filename,\"w\", newline='',encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, delimiter=\",\")\n",
    "    wr.writerow(csv_headers)\n",
    "    \n",
    "if sub_filter.lower()==\"y\":\n",
    "    gen = api.search_submissions(q=keyword, subreddit=sub_reddit, after=start_epoch)\n",
    "else:\n",
    "    gen = api.search_submissions(q=keyword, after=start_epoch)\n",
    "\n",
    "cache = []\n",
    "com_count=0\n",
    "sub_count=0\n",
    "\n",
    "csv_headers_2=[\"submission_id\", \"submission_url\", \"users\", \"subreddit\"]\n",
    "with open(filename_2,\"w\", newline='',encoding='utf-8') as q:\n",
    "    wr2 = csv.writer(q, delimiter=\",\")\n",
    "    wr2.writerow(csv_headers_2)\n",
    "\n",
    "for c in gen:\n",
    "    sub=c\n",
    "    sub_users=[]\n",
    "    sub_author=sub.author\n",
    "    sub_downs=sub.downs\n",
    "    sub_ups=sub.ups\n",
    "    sub_likes=sub.likes\n",
    "    sub_title=sub.title\n",
    "    sub_created=sub.created\n",
    "    sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    subreddit_name=str(sub.subreddit)\n",
    "    sub_id=sub.id\n",
    "    sub_reports=sub.num_reports\n",
    "    sub_text=sub.selftext\n",
    "    sub_users.append(str(sub_author))\n",
    "    sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "    sub.comments.replace_more(limit=None)\n",
    "    sub_list=sub.comments.list()\n",
    "    sub_count=sub_count+1\n",
    "    csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports, subreddit_name, sub_id]\n",
    "    with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_list)\n",
    "    for comment in sub_list:\n",
    "        comment_depth=comment.depth\n",
    "        comment_parent_id=comment.parent_id\n",
    "        comment_parent_id=comment_parent_id.split(\"_\")[1]\n",
    "        comment_reports=comment.num_reports\n",
    "        comment_author=str(comment.author)\n",
    "        if comment_author.lower() in blacklisted_users:\n",
    "            continue\n",
    "        comment_body=comment.body\n",
    "        comment_created=comment.created_utc\n",
    "        comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        comment_downs=comment.downs\n",
    "        comment_ups=comment.ups\n",
    "        comment_likes=comment.likes\n",
    "        comment_id=comment.id\n",
    "        sub_users.append(str(comment_author))\n",
    "\n",
    "        comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "        csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports, subreddit_name, sub_id]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        com_count=com_count+1\n",
    "\n",
    "    user_entry=\"\"\n",
    "    for user in sub_users:\n",
    "        if not sub_users.index(user)==len(sub_users)-1:\n",
    "            if user:\n",
    "                user_entry=user_entry+user+\";\"\n",
    "        else:\n",
    "            if user:\n",
    "                user_entry=user_entry+user\n",
    "    csv_list_2=[str(sub_id), str(sub_url), user_entry,subreddit_name]\n",
    "            \n",
    "    with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "        wr2 = csv.writer(q, delimiter=\",\")\n",
    "        wr2.writerow(csv_list_2)\n",
    "    if sub_count >= max_response_cache:\n",
    "        break\n",
    "    if sub_count % 50 == 0:\n",
    "        print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum \"+str(max_response_cache)+\". Continuing harvest...\")\n",
    "\n",
    "if False:\n",
    "    for c in gen:\n",
    "        sub=c\n",
    "        sub_users=[]\n",
    "        sub_author=sub.author\n",
    "        sub_downs=sub.downs\n",
    "        sub_ups=sub.ups\n",
    "        sub_likes=sub.likes\n",
    "        sub_title=sub.title\n",
    "        sub_created=sub.created\n",
    "        sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        subreddit_name=str(sub.subreddit)\n",
    "        sub_id=sub.id\n",
    "        sub_reports=sub.num_reports\n",
    "        sub_text=sub.selftext\n",
    "        sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        sub_list=sub.comments.list()\n",
    "        sub_count=sub_count+1\n",
    "        sub_users.append(str(sub_author))\n",
    "        csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports, subreddit_name, sub_id]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        for comment in sub_list:\n",
    "            comment_depth=comment.depth\n",
    "            comment_parent_id=comment.parent_id\n",
    "            comment_parent_id=comment_parent_id.split(\"_\")[1]\n",
    "            comment_reports=comment.num_reports\n",
    "            comment_author=str(comment.author)\n",
    "            if comment_author.lower() in blacklisted_users:\n",
    "                continue\n",
    "            comment_body=comment.body\n",
    "            comment_created=comment.created_utc\n",
    "            comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comment_downs=comment.downs\n",
    "            comment_ups=comment.ups\n",
    "            comment_likes=comment.likes\n",
    "            comment_id=comment.id\n",
    "            sub_users.append(str(comment_author))\n",
    "            \n",
    "            comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "            csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports, subreddit_name, sub_id]\n",
    "            with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "                wr = csv.writer(f, delimiter=\",\")\n",
    "                wr.writerow(csv_list)\n",
    "            com_count=com_count+1\n",
    "        user_entry=\"\"\n",
    "        for user in sub_users:\n",
    "            if not sub_users.index(user)==len(sub_users)-1:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user+\";\"\n",
    "            else:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user\n",
    "        csv_list_2=[str(sub_id), str(sub_url), user_entry,str(subreddit_name)]\n",
    "        with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "            wr2 = csv.writer(q, delimiter=\",\")\n",
    "            wr2.writerow(csv_list_2)\n",
    "        if sub_count % 50 == 0:\n",
    "            print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum \"+str(max_response_cache)+\". Continuing harvest...\")\n",
    "\n",
    "print(\"The script is done! We have harvested \"+str(sub_count)+\" submissions with \"+ str(com_count)+\" comments in total.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
