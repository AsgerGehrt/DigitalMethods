{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 1: Harvest Wikipedia category members  \n",
    "\n",
    "Wikipedia articles are arranged in categories. As an example, the __[Wikipedia Category of Circumcision](https://en.wikipedia.org/wiki/Category:Circumcision)__, encompass an array of articles such as __[Religious male circumcision](https://en.wikipedia.org/wiki/Religious_male_circumcision)__ and __[Circumcision and HIV](https://en.wikipedia.org/wiki/Circumcision_and_HIV)__ as well as subcategories such as __[Female genital mutilation](https://en.wikipedia.org/wiki/Category:Female_genital_mutilation)__.\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549392104/Mapping%20controversies%202019/Category_circumcision.jpg\" title=\"Category:circumcision\" style=\"width: 800px;\" /> \n",
    "\n",
    "\n",
    "\n",
    "This Jupyter notebook will help you harvest category members of the topic you are working on. The information will later be used generate a co-reference network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries \n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.  \n",
    "\n",
    "In order to run the installation, click on the cell below and press \"Run\" in the menu. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia library has been imported\n",
      "Wikipedia api library has been imported\n"
     ]
    }
   ],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed to carry out the harvest of data from Wikipedia\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import wikipedia\n",
    "    print(\"Wikipedia library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"Wikipedia library not found. Installing...\")\n",
    "    !pip install wikipedia\n",
    "    try:#... and try to import it again\n",
    "        import wikipedia\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the wikipedia library. Please check your internet connection and consult output from the installation below\")\n",
    "try:\n",
    "    import wikipediaapi\n",
    "    print(\"Wikipedia api library has been imported\")\n",
    "except:\n",
    "    print(\"wikipedia api library not found. Installing...\")\n",
    "    !pip install wikipedia-api\n",
    "    \n",
    "    try:\n",
    "        import wikipediaapi\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the wikipedia api library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Harvest the data from Wikipedia\n",
    "In this step, we will harvest the data from wikipedia. When you run the cell (ctrl+enter or _Run_ in menu), you will be asked to input a series of informations to the program. It is important that you do not set the depth to more than 2, as you might end up harvesting a substantial part of Wikipedia (which will take a long time, probably crash and get the AAU IP address banned from Wikipedia). \n",
    "\n",
    "Depth refers to how many steps from the starting point you wish to include. In the case of Circumcision, dept 0 would only include the articles _immediately_ related to the category, whereas dept 1 would also include articles related to the subcategories (e.g. Female genital mutilation) and dept 2 the subcategories of the subcategories (e.g. [Activists against female genital mutilation](https://en.wikipedia.org/wiki/Category:Activists_against_female_genital_mutilation)).\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549393775/Mapping%20controversies%202019/Depth.jpg\" title=\"Category:circumcision\" style=\"width: 800px;\" /> \n",
    "\n",
    "\n",
    "The script outputs two types of documents containing the same information. The first is a JSON file and the other a CSV. When the script is done, the most convenient way to view it, is by using the CSV.\n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):\n",
      "en\n",
      " \n",
      "Enter the name of the Wikipedia category you wish to scrape for member articles:\n",
      "2019–20_coronavirus_pandemic\n",
      "\n",
      "Enter the depth level between 0 and 2 you wish to query for categories:\n",
      "0\n",
      "\n",
      "Logging category members. This might take a while...\n",
      "\n",
      "There are 14 members in the category 2019–20_coronavirus_pandemic depth 0. Saving...\n",
      "\n",
      "Done harvesting. Find your files in the folder of this script:\n",
      "/c/Users/ago/Documents/Jupyter/Mapping controversies 2020/category_members_2019–20_coronavirus_pandemic_depth_0.csv\n",
      "/c/Users/ago/Documents/Jupyter/Mapping controversies 2020/category_members_2019–20_coronavirus_pandemic_depth_0.json\n"
     ]
    }
   ],
   "source": [
    "#imports 2 Python libraries for interaction with the Wikipedia API\n",
    "import wikipediaapi\n",
    "import wikipedia\n",
    "import json\n",
    "import csv\n",
    "\n",
    "#asks the user to input a language version of Wikipedia and tells the API to use that version\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "print(\" \")\n",
    "wiki_wiki = wikipediaapi.Wikipedia(lan)\n",
    "\n",
    "# asks the user to input the wikipedia category\n",
    "print('Enter the name of the Wikipedia category you wish to scrape for member articles:')\n",
    "cat_name = input()\n",
    "#cat_name=cat_name.lower()\n",
    "print(\"\")\n",
    "# asks the user to input a depth level\n",
    "print('Enter the depth level between 0 and 2 you wish to query for categories:')\n",
    "depth = input()\n",
    "print(\"\")\n",
    "depth=int(depth)\n",
    "\n",
    "print(\"Logging category members. This might take a while...\")\n",
    "print(\"\")\n",
    "\n",
    "def log_categorymembers(categorymembers, level=0):\n",
    "    for c in categorymembers.values():\n",
    "        if c.ns == 0:\n",
    "            entry = {'title':c.title,'level':level}\n",
    "            cat_members.append(entry)\n",
    "        if c.ns == 14 and level < depth:\n",
    "            log_categorymembers (c.categorymembers, level + 1)\n",
    "            \n",
    "            \n",
    "cat_members=[]\n",
    "\n",
    "cat = wiki_wiki.page(\"Category:\"+cat_name)\n",
    "log_categorymembers(cat.categorymembers)\n",
    "\n",
    "print(\"There are \"+ str(len(cat_members))+\" members in the category \"+cat_name+\" depth \"+str(depth)+\". Saving...\")\n",
    "print(\"\")\n",
    "if len(cat_name.split(\" \"))>1:\n",
    "    new_word=\"\"\n",
    "    for each in cat_name.split(\" \"):\n",
    "        if cat_name.split(\" \").index(each)==len(cat_name.split(\" \"))-1:\n",
    "            new_word=new_word+each\n",
    "        else:\n",
    "            new_word=new_word+each+\"_\"\n",
    "else:\n",
    "    new_word=cat_name\n",
    "filename='category_members_'+new_word+'_depth_'+str(depth)\n",
    "json_path = filename+'.json'\n",
    "csv_path=filename+'.csv'\n",
    "\n",
    "with open(json_path, 'w') as outfile:\n",
    "    json.dump(cat_members, outfile)\n",
    "\n",
    "headers=['Title','Level']\n",
    "\n",
    "with open(csv_path,\"w\", newline='',encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, delimiter=\",\")\n",
    "    wr.writerow(headers)\n",
    "for each in cat_members:\n",
    "    csv_list=[each[\"title\"], each[\"level\"]]\n",
    "    with open(csv_path,\"a\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_list)\n",
    "\n",
    "locale=!pwd\n",
    "print('Done harvesting. Find your files in the folder of this script:')\n",
    "print(locale[0]+\"/\"+csv_path)\n",
    "print(locale[0]+\"/\"+json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
