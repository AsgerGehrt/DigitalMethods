{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 2: Make two different networks based on all links found on a wikipedia page \n",
    "\n",
    "In the script \"MCTutorial2_Wikipedia_InText_reference_Network_final\" we looked at all the links found in the main text of a Wikipedia article. By doing so, we exclude links that has been assigned to the article based on a template. As wikipedia puts it: _\"Templates are pages that are embedded (transcluded) into other pages to allow for the repetition of information\"_ ([Wikipedia templates](https://en.wikipedia.org/wiki/Wikipedia:Templates)). The template can be found in the buttom of every Wikipedia page: \n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549631130/Mapping%20controversies%202019/Template.jpg\" title=\"Category:circumcision\" style=\"width: 700px;\" /> \n",
    "\n",
    "In this tutorial, we will include all \"internal\" links to other Wikipedia pages found on a page (i.e. the links found in the templates and in the main text). \n",
    "\n",
    "This script takes as input a file with category members from Wikipedia (e.g. \"cat_members_circumcision_depth_2.json\") and builds two networks. One network with the cat members (only) connected by the links and one with the cat-mebers + all the pages they point to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries\n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.\n",
    "\n",
    "\n",
    "__Obs: in this workbook we will be using the wikipedia and networkx libraries. If you have already installed them once, there is no need to do it again. You may simply skip to step 2.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipediaapi library has been imported\n",
      "NetworkX library has been imported\n"
     ]
    }
   ],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed \n",
    "\n",
    "import sys\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import wikipediaapi\n",
    "    print(\"wikipediaapi library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"wikipediaapi library not found. Installing...\")\n",
    "    !pip install wikipedia-api\n",
    "    try:#... and try to import it again\n",
    "        import wikipediaapi\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the wikipediaapi library. Please check your internet connection and consult output from the installation below\")\n",
    "try:\n",
    "    import networkx\n",
    "    print(\"NetworkX library has been imported\")\n",
    "except:\n",
    "    print(\"NetworkX library not found. Installing...\")\n",
    "    !pip install networkx\n",
    "    \n",
    "    try:\n",
    "        import networkx\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the NetworkX library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make the networks of all links\n",
    "\n",
    "The next step is to make the network. Here, you need to input the path to the json files you got from the MCTutorial1_Wikipedia_HarvestCatMembers_final script. \n",
    "\n",
    "If the JSON files are in the same directory as the scripts, you only need to input relational directions (i.e. the name of the json file e.g. cat_members_circumcision_depth_2)\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549444568/Mapping%20controversies%202019/Script_json_same_folder_in_text.jpg\" title=\"Folder\" style=\"width: 800px;\" /> \n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the category members json file you wish to use for keyword search (e.g.cat_members_circumcision_depth_2). If you have multiple files separate them with a comma\n",
      "category_members_2019–20_coronavirus_pandemic_depth_0\n",
      " \n",
      "Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):\n",
      "en\n",
      " \n",
      "Harvesting all links from 14 wikipedia pages. This might take a while...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import wikipediaapi\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "cat_members_all=[]\n",
    "path=\"category_members_2019–20_coronavirus_pandemic_depth_3.json\"\n",
    "with open(path) as jsonfile:\n",
    "    cat_members = json.load(jsonfile)\n",
    "    jsonfile.close()\n",
    "for every in cat_members:\n",
    "    cat_members_all.append(every[\"title\"])\n",
    "\n",
    "cat_members_all=list(set(cat_members_all))\n",
    "    \n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "print(\" \")\n",
    "wiki_wiki = wikipediaapi.Wikipedia(lan)\n",
    "Revisions = []\n",
    "count=0\n",
    "S = requests.Session()\n",
    "def do_stuff(each,page_dict,cat_member):\n",
    "    \n",
    "    title=cat_member\n",
    "    revid=each[\"revid\"]\n",
    "    \n",
    "    page_dict[title][\"revisions\"][revid]=each\n",
    "    if \"userid\" in each:\n",
    "        page_dict[title][\"users\"].append(each[\"userid\"])\n",
    "    else:\n",
    "        page_dict[title][\"users\"].append(\"0\")\n",
    "    page_dict[title][\"no_revisions\"]+=1\n",
    "page_dict={}\n",
    "print(\"Harvesting revision history...\")    \n",
    "URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
    "for cat_member in cat_members_all:\n",
    "    if cat_members_all.index(cat_member) % 100 == 0 and cat_members_all.index(cat_member)!=0:\n",
    "        print(\"The script has harvested from \"+str(cat_members_all.index(cat_member))+\" pages.\")\n",
    "        print(count)\n",
    "    page_dict[cat_member]={\"users\":[], \"no_revisions\":0,\"revisions\":{}}\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": cat_member,\n",
    "        \"rvlimit\": \"500\",\n",
    "        \"rvprop\": \"timestamp|user|userid|ids|size|type|comment|tags|flags\",\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"rvstart\": \"2020-02-02T00:00:00Z\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\"\n",
    "\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "\n",
    "\n",
    "        for each in R.json()[\"query\"][\"pages\"][0][\"revisions\"]:\n",
    "            do_stuff(each,page_dict,cat_member)\n",
    "            count+=1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    while 'continue' in R.json().keys():\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"revisions\",\n",
    "            \"titles\": cat_member,\n",
    "            \"rvlimit\": \"500\",\n",
    "            \"rvprop\": \"timestamp|user|userid|ids|size|type|comment|tags|flags\",\n",
    "            \"rvdir\": \"newer\",\n",
    "            \"rvstart\": \"2020-02-02T00:00:00Z\",\n",
    "            \"formatversion\": \"2\",\n",
    "            \"format\": \"json\",\n",
    "            \"rvcontinue\": R.json()['continue']['rvcontinue']\n",
    "\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            R = S.get(url=URL, params=PARAMS)\n",
    "            DATA = R.json()\n",
    "            for each in R.json()[\"query\"][\"pages\"][0][\"revisions\"]:\n",
    "                do_stuff(each,page_dict,cat_member)\n",
    "                count+=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "#with open(\"corona_cat_page_revisions.json\", 'w') as outfile:\n",
    "  #  json.dump(page_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path=\"corona_cat_page_revisions.json\"\n",
    "with open(path) as jsonfile:\n",
    "    page_dict = json.load(jsonfile)\n",
    "    jsonfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982\n",
      "417.20498180389404\n",
      "942\n",
      "71.44591307640076\n",
      "943\n",
      "93.79015231132507\n",
      "944\n",
      "106.90655589103699\n",
      "945\n",
      "57.627869844436646\n",
      "946\n",
      "14.405474185943604\n",
      "947\n",
      "10.623589515686035\n",
      "948\n",
      "38.267653942108154\n",
      "949\n",
      "46.41962480545044\n",
      "950\n",
      "3.694121837615967\n",
      "951\n",
      "234.70427441596985\n",
      "952\n",
      "272.7630937099457\n",
      "953\n",
      "1661.549043893814\n",
      "954\n",
      "590.4154393672943\n",
      "955\n",
      "37.596540689468384\n",
      "956\n",
      "3.951838254928589\n",
      "957\n",
      "10.126203775405884\n",
      "958\n",
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
      "2077.8182735443115\n",
      "959\n",
      "3.3934667110443115\n",
      "960\n",
      "19.36336064338684\n",
      "961\n",
      "68.20982027053833\n",
      "962\n",
      "26.821754693984985\n",
      "963\n",
      "466.38991475105286\n",
      "964\n",
      "27.741257429122925\n",
      "965\n",
      "167.84164381027222\n",
      "966\n",
      "19.812577962875366\n",
      "967\n",
      "53.48362588882446\n",
      "968\n",
      "695.4083988666534\n",
      "969\n",
      "7.197615623474121\n",
      "970\n",
      "94.55283403396606\n",
      "971\n",
      "236.06293940544128\n",
      "972\n",
      "0.7820987701416016\n",
      "973\n",
      "5.500099182128906\n",
      "974\n",
      "16.825317859649658\n",
      "975\n",
      "13.59574007987976\n",
      "976\n",
      "22.41301465034485\n",
      "977\n",
      "18.11715054512024\n",
      "978\n",
      "36.966047286987305\n",
      "979\n",
      "343.71502161026\n",
      "980\n",
      "11.597995519638062\n",
      "981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "try: \n",
    "    from BeautifulSoup import BeautifulSoup\n",
    "except ImportError:\n",
    "    from bs4 import BeautifulSoup\n",
    "root_dir=\"wiki_revision_dumps/\"\n",
    "Revisions = []\n",
    "lan=\"en\"\n",
    "S = requests.Session()\n",
    "    \n",
    "    \n",
    "URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
    "pages=list(page_dict.keys())\n",
    "print(len(pages))\n",
    "for page in pages[941:]:\n",
    "    start_time=time.time()\n",
    "    if \"/\" in page:\n",
    "        page.replace(\"/\",\" \")\n",
    "    if page+\".json\" in os.listdir(root_dir):\n",
    "        continue\n",
    "    dump_dict=page_dict[page]\n",
    "    revisions=page_dict[page][\"revisions\"]\n",
    "    for revision in revisions:\n",
    "        PARAMS = {\n",
    "            \"action\": \"parse\",\n",
    "            \"oldid\":revision,\n",
    "            \"prop\":\"externallinks|categories|iwlinks|text\", \n",
    "            \"format\": \"json\"\n",
    "\n",
    "        }\n",
    "        try:\n",
    "            R = S.get(url=URL, params=PARAMS)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        if not \"parse\" in R.json():\n",
    "            continue\n",
    "        html=R.json()[\"parse\"][\"text\"][\"*\"]\n",
    "\n",
    "        #the HTML code you've written above\n",
    "        parsed_html = BeautifulSoup(html)\n",
    "        parsed_text=parsed_html.text\n",
    "        if \"References\" in parsed_text:\n",
    "            split_text=parsed_text.split(\"References\")\n",
    "            if len(split_text)!=2:\n",
    "                new_split_text=\"\"\n",
    "                for each in split_text[:len(split_text)-1]:\n",
    "                    new_split_text+=each\n",
    "\n",
    "            else:\n",
    "                new_split_text=split_text\n",
    "        else:\n",
    "            new_split_text=parsed_text\n",
    "        text_links = []\n",
    "        html = str(parsed_html).split('<p>')\n",
    "        for p in html[1:]:\n",
    "            p = p.split('</p>')[0]\n",
    "            links = p.split('<a href=\"')\n",
    "            for l in links[1:]:\n",
    "                if(' title=\"') in l:\n",
    "                    l = l.split(' title=\"')[1]\n",
    "                    l = l.split('\">')[0]\n",
    "                    l = l.replace(\"&#39;\",\"'\")\n",
    "                    if l not in text_links:\n",
    "                        text_links.append(l)\n",
    "        dump_dict[\"revisions\"][revision][\"in_text_links\"]=text_links\n",
    "        dump_dict[\"revisions\"][revision][\"text\"]=new_split_text\n",
    "        parse=R.json()[\"parse\"]\n",
    "        dump_dict[\"revisions\"][revision][\"categories\"]=parse[\"categories\"]\n",
    "        dump_dict[\"revisions\"][revision][\"external_links\"]=parse[\"externallinks\"]\n",
    "        dump_dict[\"revisions\"][revision][\"inter_wiki_links\"]=parse[\"iwlinks\"]\n",
    "\n",
    "    with open(root_dir+page+\".json\", 'w') as outfile:\n",
    "        json.dump(dump_dict, outfile)\n",
    "    print(time.time()-start_time)\n",
    "    print(pages.index(page))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No references\n",
      "This article may be affected by a current event. Information in this article may change rapidly as the event progresses. Initial news reports may be unreliable. The last updates to this article may not reflect the most current information. Please feel free to improve this article (but note that updates without valid and reliable references will be removed) or discuss changes on the talk page. (Learn how and when to remove this template message)\n",
      "2019-2020 China pneumonia outbreak, or China pnumonia, commonly known as Wuhan pnumia (Chinese: 武漢肺炎; pinyin: wǔhàn fèiyán) or pneumonia of unknown origin (Chinese: 不明原因肺炎; pinyin: bùmíng yuányīn fèiyán), is the pneumonia outbreak firstly discovered in Huanan Seafood Market in Wuhan, China. [1][2]</nowiki>\n",
      "^ \"Mystery pneumonia virus probed in China\". 2020-01-03. Retrieved 2020-01-05..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}\n",
      "\n",
      "^ \"China Pneumonia Outbreak Spurs WHO Action as Mystery Lingers\". www.bloomberg.com. 2020-01-04. Retrieved 2020-01-05.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No references\n",
      "2019-nCoV Acute Respiratory Syndrome is a viral respiratory disease of zoonotic origin caused by the 2019 novel coronavirus (2019-nCoV).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No references\n",
      "The Wuhan coronavirus is a novel coronavirus first reported in 2020 and sequenced after nucleic acid testing on a positive patient sample.[1]\n",
      "^ \"新型冠状病毒！武汉不明原因肺炎\"元凶\"初步判定_新闻_央视网(cctv.com)\". m.news.cctv.com. Retrieved 2020-01-09..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No references\n",
      "Testing for COVID 19 the illness and the associated virus can be done by PCR nucleic acid-based tests and antibody test kits. \n",
      "China has announced it is making 1.7 million test kits a day as or 25 Feb 2020.[1]\n",
      "\n",
      "The British NHS has announced that it will start testing suspected cases at home which saves the risk of infecting others if they come to a hospital and having to disinfect the ambulance if one is used.[2]\n",
      "^  https://yicaiglobal.com/news/china-makes-over-17-million-covid-19-testing-kits-per-day-official-says. Missing or empty |title= (help).mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}\n",
      "\n",
      "^ \"NHS pilots home testing for coronavirus\". MobiHealthNews. 24 February 2020.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "No references\n",
      "The coronavirus vaccine is an anticipated vaccine under development as of 2020, with the expectation that it will ultimately yield a defense against Coronavirus disease 2019.\n",
      "\n",
      "Many organizations are using published genomes to develop possible vaccines against SARS-CoV-2.[1][2] Bodies developing vaccines include the Chinese Center for Disease Control and Prevention,[3][4] the University of Hong Kong,[5] the Shanghai East Hospital,[5] and other universities, such as Washington University in St. Louis.[6]\n",
      "^ Cite error: The named reference Reut_NIH_Moderna_3months was invoked but never defined (see the help page).\n",
      "\n",
      "^ Cite error: The named reference clinicaltrialsarena was invoked but never defined (see the help page).\n",
      "\n",
      "^ \"China CDC developing novel coronavirus vaccine\". Xinhua. 26 January 2020. Archived from the original on 26 January 2020. Retrieved 28 January 2020..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}\n",
      "\n",
      "^ \"Chinese scientists race to develop vaccine as coronavirus death toll jumps\". South China Morning Post. 26 January 2020. Archived from the original on 26 January 2020. Retrieved 28 January 2020.\n",
      "\n",
      "^ a b Cheung E (28 January 2020). \"Hong Kong researchers have developed coronavirus vaccine, expert reveals\". South China Morning Post. Archived from the original on 28 January 2020. Retrieved 28 January 2020.\n",
      "\n",
      "^ Chen, Eli. \"Wash U Scientists Are Developing A Coronavirus Vaccine\". news.stlpublicradio.org. Retrieved 2020-03-06.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "try: \n",
    "    from BeautifulSoup import BeautifulSoup\n",
    "except ImportError:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "Revisions = []\n",
    "lan=\"en\"\n",
    "S = requests.Session()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
    "for page in page_dict:\n",
    "    for revision in page_dict[page][\"revisions\"]:\n",
    "        PARAMS = {\n",
    "            \"action\": \"parse\",\n",
    "            \"oldid\":revision,\n",
    "            \"prop\":\"externallinks|categories|iwlinks|text\", \n",
    "            \"preview\":True,\n",
    "            \"format\": \"json\"\n",
    "\n",
    "        }\n",
    "\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        html=R.json()[\"parse\"][\"text\"][\"*\"]\n",
    "\n",
    "        #the HTML code you've written above\n",
    "        parsed_html = BeautifulSoup(html)\n",
    "        parsed_text=parsed_html.text\n",
    "        if \"References\" in parsed_text:\n",
    "            split_text=parsed_text.split(\"References\")\n",
    "            if len(split_text)!=2:\n",
    "                new_split_text=\"\"\n",
    "                for each in split_text[:len(split_text)-1]:\n",
    "                    new_split_text+=each\n",
    "              \n",
    "            else:\n",
    "                new_split_text=split_text\n",
    "        else:\n",
    "            new_split_text=parsed_text\n",
    "\n",
    "        page_dict[page][\"revisions\"][revision][\"text\"]=new_split_text\n",
    "        parse=R.json()[\"parse\"]\n",
    "        page_dict[page][\"revisions\"][revision][\"categories\"]=parse[\"categories\"]\n",
    "        page_dict[page][\"revisions\"][revision][\"externallinks\"]=parse[\"externallinks\"]\n",
    "        page_dict[page][\"revisions\"][revision][\"iwlinks\"]=parse[\"iwlinks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'pageid', 'revid', 'text', 'categories', 'externallinks', 'iwlinks'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.json()[\"parse\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "html = text#the HTML code you've written above\n",
    "parsed_html = BeautifulSoup(html)\n",
    "parsed_text=parsed_html.text\n",
    "if \"References\" in parsed_text:\n",
    "    print(len(parsed_text.split(\"References\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "if \"References\" in parsed_text:\n",
    "    print(len(parsed_text.split(\"References\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Category:2010s medical outbreaks', 'Category:January 2020 events', 'Category:December 2019 events']\n",
      "['Wuhan', 'China']\n",
      "['https://www.bbc.com/news/world-asia-china-50984025', 'https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=']\n",
      "{{Current related}}\n",
      "'''2019-2020 China pneumonia outbreak''', or '''China pnumonia''', commonly known as '''Wuhan pnumia''' ({{Lang-zh|c='''武漢肺炎'''|s=|t=|p=wǔhàn fèiyán}}) or '''pneumonia of unknown origin''' ({{Lang-zh|c='''不明原因肺炎'''|s=|t=|p=bùmíng yuányīn fèiyán}}), is the pneumonia outbreak firstly discovered in Huanan Seafood Market in [[Wuhan]], [[China]]. <ref>{{Cite news|url=https://www.bbc.com/news/world-asia-china-50984025|title=Mystery pneumonia virus probed in China|date=2020-01-03|access-date=2020-01-05|language=en-GB}}</ref><ref>{{Cite web|url=https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=|title=China Pneumonia Outbreak Spurs WHO Action as Mystery Lingers|last=|first=|date=2020-01-04|website=www.bloomberg.com|url-status=live|archive-url=|archive-date=|access-date=2020-01-05}}</ref>\n",
      "\n",
      "[[Category:2010s medical outbreaks]]\n",
      "[[Category:January 2020 events]]\n",
      "[[Category:December 2019 events]]</nowiki>\n",
      "__________\n",
      "['Category:2010s medical outbreaks', 'Category:January 2020 events', 'Category:December 2019 events']\n",
      "['Wuhan', 'China']\n",
      "['https://www.bbc.com/news/world-asia-china-50984025', 'https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=']\n",
      "{{Current related}}\n",
      "'''2019-2020 China pneumonia outbreak''', or '''China pnumonia''', commonly known as '''Wuhan pnumia''' ({{Lang-zh|c='''武漢肺炎'''|s=|t=|p=wǔhàn fèiyán}}) or '''pneumonia of unknown origin''' ({{Lang-zh|c='''不明原因肺炎'''|s=|t=|p=bùmíng yuányīn fèiyán}}), is the pneumonia outbreak firstly discovered in Huanan Seafood Market in [[Wuhan]], [[China]]. <ref>{{Cite news|url=https://www.bbc.com/news/world-asia-china-50984025|title=Mystery pneumonia virus probed in China|date=2020-01-03|access-date=2020-01-05|language=en-GB}}</ref><ref>{{Cite web|url=https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=|title=China Pneumonia Outbreak Spurs WHO Action as Mystery Lingers|last=|first=|date=2020-01-04|website=www.bloomberg.com|url-status=live|archive-url=|archive-date=|access-date=2020-01-05}}</ref>\n",
      "\n",
      "[[Category:2010s medical outbreaks]]\n",
      "[[Category:January 2020 events]]\n",
      "[[Category:December 2019 events]]\n",
      "__________\n",
      "['Category:2010s medical outbreaks', 'Category:January 2020 events', 'Category:December 2019 events']\n",
      "['Wuhan', 'China']\n",
      "['https://www.bbc.com/news/world-asia-china-50984025', 'https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=']\n",
      "{{Current related}}\n",
      "'''2019-2020 China pneumonia outbreak''', or '''China pnumonia''', commonly known as '''Wuhan pnumia''' ({{Lang-zh|c='''武漢肺炎'''|s=|t=|p=wǔhàn fèiyán}}) or '''pneumonia of unknown origin''' ({{Lang-zh|c='''不明原因肺炎'''|s=|t=|p=bùmíng yuányīn fèiyán}}), is the pneumonia outbreak firstly discovered in Huanan Seafood Market in [[Wuhan]], [[China]]. <ref>{{Cite news|url=https://www.bbc.com/news/world-asia-china-50984025|title=Mystery pneumonia virus probed in China|date=2020-01-03|access-date=2020-01-05|language=en-GB}}</ref><ref>{{Cite web|url=https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=|title=China Pneumonia Outbreak Spurs WHO Action as Mystery Lingers|last=|first=|date=2020-01-04|website=www.bloomberg.com|url-status=live|archive-url=|archive-date=|access-date=2020-01-05}}</ref>\n",
      "\n",
      "[[Category:2010s medical outbreaks]]\n",
      "[[Category:January 2020 events]]\n",
      "[[Category:December 2019 events]]\n",
      "__________\n",
      "['Category:2010s medical outbreaks', 'Category:January 2020 events', 'Category:December 2019 events']\n",
      "['Wuhan', 'China']\n",
      "['https://www.bbc.com/news/world-asia-china-50984025', 'https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=']\n",
      "{{Current related}}\n",
      "'''2019-2020 China pneumonia outbreak''', or '''China pneumonia''', commonly known as '''Wuhan pneumonia''' ({{Lang-zh|c=武漢肺炎|s=|t=|p=wǔhàn fèiyán}}) or '''pneumonia of unknown origin''' ({{Lang-zh|c=不明原因肺炎|s=|t=|p=bùmíng yuányīn fèiyán}}), is a pneumonia outbreak firstly discovered in Huanan Seafood Market in [[Wuhan]], [[China]].<ref>{{Cite news|url=https://www.bbc.com/news/world-asia-china-50984025|title=Mystery pneumonia virus probed in China|date=2020-01-03|access-date=2020-01-05|language=en-GB}}</ref><ref>{{Cite web|url=https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=|title=China Pneumonia Outbreak Spurs WHO Action as Mystery Lingers|last=|first=|date=2020-01-04|website=www.bloomberg.com|url-status=live|archive-url=|archive-date=|access-date=2020-01-05}}</ref>\n",
      "\n",
      "==References==\n",
      "{{reflist}}\n",
      "\n",
      "[[Category:2010s medical outbreaks]]\n",
      "[[Category:January 2020 events]]\n",
      "[[Category:December 2019 events]]\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "for each in R.json()[\"query\"][\"pages\"][0][\"revisions\"][:4]:\n",
    "    content=each[\"content\"]\n",
    "    refs=re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', content)\n",
    "    regex='\\[(.*?)\\]'\n",
    "    cats=[]\n",
    "    links=[]\n",
    "    for cat in re.findall(regex,content):\n",
    "\n",
    "        if \"Category\" in cat:\n",
    "            cats.append(cat.strip(\"[\").strip(\"]\"))\n",
    "        else:\n",
    "            links.append(cat.strip(\"[\").strip(\"]\"))\n",
    "    print(cats)\n",
    "    print(links)\n",
    "    print(refs)\n",
    "    print(content)\n",
    "    print(\"__________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.bbc.com/news/world-asia-china-50984025',\n",
       " 'https://www.bloomberg.com/tosv2.html?vid=&uuid=371a3210-2fd3-11ea-8928-dd02747387de&url=L25ld3MvYXJ0aWNsZXMvMjAyMC0wMS0wNC9jaGluYS1wbmV1bW9uaWEtb3V0YnJlYWstc3B1cnMtd2hvLWFjdGlvbi1hcy1teXN0ZXJ5LWxpbmdlcnM=']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:2010s medical outbreaks\n",
      "Category:January 2020 events\n",
      "Category:December 2019 events\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-32b0fb7054ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrefs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"refs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "refs=each[\"content\"].split(\"refs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1\n",
    "for each in cat_members_all:\n",
    "    title=each[\"title\"]\n",
    "    if count % 50 == 0:\n",
    "        print(\"All links harvested from \"+str(count)+\" pages out of \"+str(len(cat_members_all))+\". Continuing harvest...\")\n",
    "    if not title in seen:\n",
    "        seen.append(title)\n",
    "        try:\n",
    "        \n",
    "            page=wiki_wiki.page(title)\n",
    "            text_links = []\n",
    "            links = page.links\n",
    "            for link_title in sorted(links.keys()):\n",
    "                text_links.append(link_title)\n",
    "            network.update({title:text_links})\n",
    "\n",
    "        except:\n",
    "            print('SKIPPED: '+title)\n",
    "            print(\"\")\n",
    "    count=count+1\n",
    "    \n",
    "print(\"All pages harvested...\")\n",
    "new_cat_members={}\n",
    "for each in cat_members_all:\n",
    "    new_cat_members[each[\"title\"]]={\"level\":each[\"level\"]}\n",
    "    \n",
    "membersonly_edges = []\n",
    "all_edges = []\n",
    "members = network.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating networks...\")\n",
    "print(\"\")\n",
    "for source in network:\n",
    "    for target in network[source]:\n",
    "        edge = (source,target)\n",
    "        all_edges.append(edge)\n",
    "        if target in members:\n",
    "            membersonly_edges.append(edge)\n",
    "print(\"Saving networks...\")\n",
    "print(\"\")\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(membersonly_edges)\n",
    "nx.write_gexf(G,'MCTutorial2_2_'+ filename+'_AllLinksNet_membersonly.gexf')\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(all_edges)\n",
    "for each in G.nodes:\n",
    "    if each in members:\n",
    "        G.nodes[each]['member_level'] = 'Level '+str(new_cat_members[each][\"level\"])\n",
    "    else:\n",
    "        G.nodes[each]['member_level'] = 'Not a member'\n",
    "nx.write_gexf(G, 'MCTutorial2_2_'+filename+'_AllLinksNet_allpages.gexf')\n",
    "print(\"The script is done. You can find your network files by following these paths: \")\n",
    "print(\"\")\n",
    "locale=!pwd\n",
    "print(locale[0]+\"/\"+'MCTutorial2_2_'+filename+'_AllLinksNet_membersonly.gexf')\n",
    "print(\"\")\n",
    "print(locale[0]+\"/\"+'MCTutorial2_2_'+filename+'_AllLinksNet_allpages.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
