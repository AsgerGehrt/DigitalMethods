{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 5: Keyword search in articles  \n",
    "\n",
    "In this script you can specify an array of articles (either individual or by category members files), and search for keywords. The script will output a csv file with the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries\n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.\n",
    "\n",
    "\n",
    "__Obs: in this workbook we will be using the wikipedia library. If you have already installed it once, there is no need to do it again. You may simply skip to step 2.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia api library has been imported\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import wikipediaapi\n",
    "    print(\"Wikipedia api library has been imported\")\n",
    "except:\n",
    "    print(\"wikipedia api library not found. Installing...\")\n",
    "    !pip install wikipedia-api\n",
    "    \n",
    "    try:\n",
    "        import wikipediaapi\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the wikipedia api library. Please check your internet connection and consult output from the installation below\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make the queries \n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you want to input the pages for the keyword search?\n",
      "Enter '1' if you want to use a category members json file.\n",
      "Enter '2' if you want to enter the pages manually.\n",
      "Enter '0' if you want to use category members json file AND enter pages manually.\n",
      "1\n",
      "Enter the name of the category members json file you wish to use for keyword search (e.g.cat_members_circumcision_depth_2). If you have multiple files separate them with a comma\n",
      "category_members_Theory_of_cryptography_depth_2\n",
      " \n",
      "Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):\n",
      "en\n",
      "Enter the keyword(s) you would like to query for. If more than one, use comma separation. Note, that the script will not differentiate between lower and capital letters.\n",
      "differential privacy\n",
      "Do you want to use wild card in the end of the keyword (y/n)? (e.g. keyword adult will return adult, adults, adulthood etc.)\n",
      "y\n",
      "Collecting and analyzing text from 95 pages...\n",
      "\n",
      "Your search is over. \n",
      "The keyword differential privacy appeared 96 times in total.\n",
      "\n",
      "Saving CSV...\n",
      "CSV file saved. You can find the network by following this path: \n",
      "/c/Users/ago/Documents/Jupyter/Mapping controversies 2020/differential privacy_KeywordSearchInArticles.csv\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import csv\n",
    "import json\n",
    "print(\"How do you want to input the pages for the keyword search?\")\n",
    "print(\"Enter '1' if you want to use a category members json file.\")\n",
    "print(\"Enter '2' if you want to enter the pages manually.\")\n",
    "print(\"Enter '0' if you want to use category members json file AND enter pages manually.\")\n",
    "pages=[]\n",
    "input_style=input()\n",
    "#input_style=2\n",
    "if input_style==str(1) or input_style==1 or input_style==0 or input_style==str(0):\n",
    "    print(\"Enter the name of the category members json file you wish to use for keyword search (e.g.cat_members_circumcision_depth_2). If you have multiple files separate them with a comma\")\n",
    "    filename= input()\n",
    "    if \",\" in filename:\n",
    "        \n",
    "        for each in filename.split(\",\"):\n",
    "            \n",
    "\n",
    "            if not each.endswith(\".json\"):\n",
    "                path=each+\".json\"\n",
    "            else: \n",
    "                path=each\n",
    "                each=each.split(\".\")[0]\n",
    "            with open(path) as jsonfile:\n",
    "                cat_members = json.load(jsonfile)\n",
    "                jsonfile.close()\n",
    "            for every in cat_members:\n",
    "                pages.append(every['title'])\n",
    "    else:\n",
    "        print(\" \")\n",
    "        \n",
    "\n",
    "        if not filename.endswith(\".json\"):\n",
    "            path=filename+\".json\"\n",
    "        else: \n",
    "            path=filename\n",
    "            filename=filename.split(\".\")[0]\n",
    "        with open(path) as jsonfile:\n",
    "            cat_members = json.load(jsonfile)\n",
    "            jsonfile.close()\n",
    "        for each in cat_members:\n",
    "            pages.append(each['title'])\n",
    "    \n",
    "if input_style==str(2) or input_style==2 or input_style==0 or input_style==str(0):\n",
    "    print(\"Enter the names of the pages you wish to use for keyword search. If multiple pages use comma separation (e.g. circumcision,Female genital mutilation etc)\")\n",
    "    raw_input=input()\n",
    "    #raw_input=\"circumcision\"\n",
    "    if \",\" in raw_input:\n",
    "        for each in raw_input.split(\",\"):\n",
    "            pages.append(each)\n",
    "    else:\n",
    "        pages.append(raw_input)\n",
    "\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language=lan,\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "print(\"Enter the keyword(s) you would like to query for. If more than one, use comma separation. Note, that the script will not differentiate between lower and capital letters.\")\n",
    "\n",
    "keywords=input()\n",
    "#keywords=\"HIV,HPV\"\n",
    "\n",
    "print(\"Do you want to use wild card in the end of the keyword (y/n)? (e.g. keyword adult will return adult, adults, adulthood etc.)\")\n",
    "wildcard_end=input().lower()\n",
    "\n",
    "keyword_list=[]\n",
    "\n",
    "if \",\" in keywords:\n",
    "    for each in keywords.split(\",\"):\n",
    "        keyword_list.append(each.strip().lower())\n",
    "else:\n",
    "    keyword_list.append(keywords.strip().lower())\n",
    "prefix=\"\"\n",
    "for keyword in keyword_list:\n",
    "    prefix=prefix+keyword+\"_\"\n",
    "filename=prefix+\"KeywordSearchInArticles.csv\"\n",
    "\n",
    "page_dict={}\n",
    "keyword_dict={}\n",
    "print(\"Collecting and analyzing text from \"+str(len(pages))+\" pages...\")\n",
    "for keyword in keyword_list:\n",
    "    keyword_dict[keyword]=0\n",
    "for page in pages:\n",
    "    p_wiki = wiki_wiki.page(page)\n",
    "    page_text=p_wiki.text.lower()\n",
    "\n",
    "    for punc in page_text:\n",
    "        if punc==',' or punc=='.':\n",
    "            page_text=page_text.replace(punc, \" \")\n",
    "    page_dict[page]={\"keywords\":{}}\n",
    "    for keyword in keyword_list:\n",
    "        \n",
    "        if wildcard_end==\"n\":\n",
    "            new_keyword=\" \"+keyword+\" \"\n",
    "        else:\n",
    "            new_keyword=\" \"+keyword\n",
    "        keyword_count=page_text.count(new_keyword)\n",
    "        keyword_dict[keyword]=keyword_dict[keyword]+keyword_count\n",
    "        page_dict[page][\"keywords\"][keyword.strip()]=keyword_count\n",
    "print(\"\")\n",
    "print(\"Your search is over. \")\n",
    "\n",
    "for keyword in keyword_dict:\n",
    "    print(\"The keyword \"+keyword+\" appeared \"+str(keyword_dict[keyword])+\" times in total.\")\n",
    "    print(\"\")\n",
    "print(\"Saving CSV...\")\n",
    "\n",
    "headers=[\"id\"]\n",
    "\n",
    "csv_path=filename\n",
    "\n",
    "for each in keyword_list: \n",
    "    headers.append(each)\n",
    "\n",
    "with open(csv_path,\"w\", newline='',encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, delimiter=\",\")\n",
    "    wr.writerow(headers)\n",
    "for page in page_dict:\n",
    "    csv_list=[page]\n",
    "    for each in keyword_list:\n",
    "        entry=page_dict[page][\"keywords\"][each]\n",
    "        csv_list.append(entry)\n",
    "    with open(csv_path,\"a\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_list)\n",
    "print('CSV file saved. You can find the network by following this path: ')\n",
    "locale=!pwd\n",
    "print(locale[0]+\"/\"+filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
