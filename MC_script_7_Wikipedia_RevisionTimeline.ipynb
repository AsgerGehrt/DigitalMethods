{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 7: Harvest revision timeline from Wikipedia pages\n",
    "\n",
    "In Wikipedias __[revision history](https://en.wikipedia.org/w/index.php?title=Circumcision&action=history)__ you can study how users delete, append and alter content on the pages. Each alteration is logged with timestamp, user information and a complete track of the changes made.  \n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549464567/Mapping%20controversies%202019/revision_history_cir.jpg\" title=\"Category:circumcision\" style=\"width: 800px;\" /> \n",
    "\n",
    "We can reuse this information to indicate when a topic is disputed and by whom. \n",
    "\n",
    "In this tutorial we will harvest the revision timelines of the topics you are working on.  \n",
    "\n",
    "The script will output two csv files. The first includes all revisions (one revision per row) and the other counts how many revisions are made per time-period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries\n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.\n",
    "\n",
    "\n",
    "__Obs: in this workbook we will be using the requests library. If you have already installed it once, there is no need to do it again. You may simply skip to step 2.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed \n",
    "\n",
    "import sys\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import requests\n",
    "    print(\"Requests library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"Requests library not found. Installing...\")\n",
    "    !pip install requests\n",
    "    try:#... and try to import it again\n",
    "        import requests\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the requests library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import geolite2\n",
    "    print(\"geolite2 library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"geolite2 library not found. Installing...\")\n",
    "    !pip install maxminddb-geolite2\n",
    "    try:#... and try to import it again\n",
    "        import geolite2\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the geolite2 library. Please check your internet connection and consult output from the installation below\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Harvest revision timeline from Wikipedia\n",
    "\n",
    "The Wikipedia API for revisions hosts an array of different data points as you can see in the image below. For in this script, we only harvest timestamp, user, comment, slotsize, userid, ids and tags. You can think about what the other data points might be used in your controversy mapping project.      \n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549470946/Mapping%20controversies%202019/revision_params.jpg\" title=\"Category:circumcision\" style=\"width: 500px;\" /> \n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu.\n",
    "\n",
    "You can see the documentation for the revision API here: https://www.mediawiki.org/wiki/API:Revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "from geolite2 import geolite2\n",
    "reader = geolite2.reader()\n",
    "\n",
    "Revisions = []\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "print('Enter the name of the Wikipedia page you wish to query for revisions ')\n",
    "page = input()\n",
    "#page=\"circumcision\"\n",
    "\n",
    "print(\"Enter start date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2001-01-01\")\n",
    "input_date=input()\n",
    "if not input_date:\n",
    "    start_date=\"2001-01-01\"\n",
    "else:\n",
    "    start_date=input_date\n",
    "\n",
    "print(\"Define how you wan't to count the data (year or month). Leave blank to use default: month\")\n",
    "input_count=input()\n",
    "if input_count:\n",
    "    count_type=input_count\n",
    "else:\n",
    "    count_type=\"month\"\n",
    "\n",
    "print(\"Harvesting revision history...\")    \n",
    "URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"revisions\",\n",
    "    \"titles\": page,\n",
    "    \"rvlimit\": \"500\",\n",
    "    \"rvprop\": \"timestamp|user|comment|slotsize|userid|ids|tags\",\n",
    "    \"rvdir\": \"newer\",\n",
    "    \"rvstart\": start_date+\"T00:00:00Z\",\n",
    "    \"formatversion\": \"2\",\n",
    "    \"format\": \"json\"\n",
    "\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "if R.status_code==404:\n",
    "    print(\"The page does not exist\")\n",
    "DATA = R.json()\n",
    "for each in DATA['query']['pages']:\n",
    "    Revisions.append(each)\n",
    "\n",
    "while 'continue' in DATA.keys():\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": page,\n",
    "        \"rvlimit\": \"500\",\n",
    "        \"rvprop\": \"timestamp|user|comment|slotsize|userid|ids|tags\",\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"rvstart\": start_date+\"T00:00:00Z\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\",\n",
    "        \"rvcontinue\": DATA['continue']['rvcontinue']\n",
    "\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    for each in DATA['query']['pages']:\n",
    "        Revisions.append(each)\n",
    "revisions=[]\n",
    "\n",
    "for each in Revisions:\n",
    "    if \"revisions\" in each:\n",
    "        for every in each[\"revisions\"]:\n",
    "            if not \"user\" in every:\n",
    "                every[\"user\"]=\"n/a\"\n",
    "            if not \"userid\" in every:\n",
    "                every[\"userid\"]=\"n/a\"\n",
    "            if not \"comment\" in every:\n",
    "                every[\"comment\"]=\"n/a\"\n",
    "            if not \"slotsize\" in every:\n",
    "                every[\"slotsize\"]=\"n/a\"\n",
    "            if not \"tags\" in every:\n",
    "                every[\"tags\"]=\"n/a\"\n",
    "            if not \"revid\" in every:\n",
    "                every[\"revid\"]=\"n/a\"\n",
    "            if not \"parentid\" in every:\n",
    "                every[\"parentid\"]=\"n/a\"\n",
    "\n",
    "            \n",
    "            revisions.append(every)\n",
    "\n",
    "filename='MC_script_7_'+'revisions_'+page+'_from_'+start_date+'_to_now'\n",
    "json_path = filename+'_all.json'\n",
    "csv_path=filename+'_all.csv'\n",
    "\n",
    "print(\"Done harvesting revision history. Looking up geolocations for anonymous users\")\n",
    "for each in revisions: \n",
    "    user=each[\"user\"]\n",
    "    if re.match(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\",user):\n",
    "        try:\n",
    "            geo=(reader.get(user))\n",
    "            if \"location\" in geo: \n",
    "                lat=geo[\"location\"][\"latitude\"]\n",
    "                long=geo[\"location\"][\"longitude\"]\n",
    "            else: \n",
    "                lat=\"\"\n",
    "                long=\"\"\n",
    "        except:\n",
    "            lat=\"\"\n",
    "            long=\"\"\n",
    "    else:\n",
    "        lat=\"\"\n",
    "        long=\"\"\n",
    "    each[\"latitude\"]=lat\n",
    "    each[\"longitude\"]=long\n",
    "    \n",
    "with open(json_path, 'w') as outfile:\n",
    "    json.dump(revisions, outfile)\n",
    "\n",
    "headers=['revision_id','parent_id', 'user_name', 'user_id', 'timestamp', 'yyyy-mm-dd','yyyy','mm','dd','size','comment', 'tags','latitude','longitude']\n",
    "csv_list=[headers]\n",
    "\n",
    "for each in revisions:\n",
    "    date=each['timestamp'].split('T')[0]\n",
    "    year=date.split('-')[0]\n",
    "    month=date.split('-')[1]\n",
    "    day=date.split('-')[2]\n",
    "    entry=[each[\"revid\"], each[\"parentid\"],each[\"user\"],each[\"userid\"],each[\"timestamp\"],date,year,month,day,each[\"size\"],each[\"comment\"],each[\"tags\"],each[\"latitude\"],each[\"longitude\"]]\n",
    "    csv_list.append(entry)\n",
    "\n",
    "with open(csv_path,\"w\", newline='',encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, delimiter=\";\")\n",
    "    wr.writerows(csv_list)        \n",
    "start_year=int(start_date.split(\"-\")[0])\n",
    "years=[]\n",
    "for each in range(start_year,2021):\n",
    "    years.append(str(each))\n",
    "    \n",
    "months=['01', '02','03','04','05','06','07','08','09','10','11','12']\n",
    "\n",
    "quarters=[]\n",
    "\n",
    "dict_of_years={}\n",
    "csv_path_count=filename+'_count_'+count_type+'.csv'\n",
    "headers=[\"Time period (\"+count_type+\")\", \"Revision count\", \"Unique users count\", \"Title\"]\n",
    "with open(csv_path_count,\"w\", newline='',encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, delimiter=\",\")\n",
    "    wr.writerow(headers)\n",
    "for year in years:\n",
    "\n",
    "    dict_of_years[year]={}\n",
    "    if not year==\"2020\":\n",
    "        for month in months:\n",
    "            dict_of_years[year][month]={\"users\":[]}\n",
    "    else: \n",
    "        for month in months[:int(str(datetime.datetime.now()).split(\"-\")[1])]:\n",
    "            dict_of_years[year][month]={\"users\":[]}\n",
    "json_path_count=filename+'_count_'+count_type+'.json'\n",
    "\n",
    "\n",
    "\n",
    "for revision in revisions:\n",
    "    timestamp=revision[\"timestamp\"]\n",
    "    user_id=revision[\"userid\"]\n",
    "    year=timestamp.split('-')[0]\n",
    "    month=timestamp.split('-')[1]\n",
    "    dict_of_years[year][month][\"users\"].append(user_id)\n",
    "with open(json_path_count, 'w') as outfile:\n",
    "    json.dump(dict_of_years, outfile)\n",
    "csv_list=[]\n",
    "if count_type.lower()==\"month\":\n",
    "    for year in dict_of_years:\n",
    "        for month in dict_of_years[year]:\n",
    "            entry=[year+\"-\"+month, len(dict_of_years[year][month][\"users\"]), len(set(dict_of_years[year][month][\"users\"])),page ]\n",
    "            csv_list.append(entry)\n",
    "\n",
    "\n",
    "if count_type.lower()==\"year\":\n",
    "    for year in dict_of_years:\n",
    "        users=[]\n",
    "        \n",
    "        for month in dict_of_years[year]:\n",
    "            users=users+dict_of_years[year][month][\"users\"]\n",
    "        entry=[year, len(users), len(set(users)),page ]\n",
    "        csv_list.append(entry)\n",
    "with open(csv_path_count,\"a\", newline='',encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, delimiter=\",\")\n",
    "    wr.writerows(csv_list)\n",
    "locale=!pwd\n",
    "print(\"The script is done. Find the outputs here:\")\n",
    "print(locale[0]+\"/\"+csv_path_count)\n",
    "print(locale[0]+\"/\"+json_path_count)\n",
    "print(locale[0]+\"/\"+csv_path)\n",
    "print(locale[0]+\"/\"+json_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvest revision timeline for all pages in one or multiple category json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "from geolite2 import geolite2\n",
    "reader = geolite2.reader()\n",
    "\n",
    "Revisions = []\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "print('Enter the name of the Wikipedia category json you wish to query for revisions (e.g.category_members_circumcision_depth_2). If you want to harvest from multiple category json files, separate them with a comma')\n",
    "cat = input()\n",
    "#page=\"circumcision\"\n",
    "pages=[]\n",
    "if \",\" in cat:\n",
    "    for each in cat.split(\",\"):\n",
    "        if each:\n",
    "            each=each.strip()\n",
    "            if not each.endswith(\".json\"):\n",
    "                path=each+\".json\"\n",
    "            else: \n",
    "                path=each\n",
    "                each=each.split(\".\")[0]\n",
    "\n",
    "            with open(path) as jsonfile:\n",
    "                cat_members = json.load(jsonfile)\n",
    "                jsonfile.close()\n",
    "            \n",
    "            for every in cat_members:\n",
    "                pages.append(every[\"title\"])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "else:\n",
    "    if not cat.endswith(\".json\"):\n",
    "        path=cat+\".json\"\n",
    "    else: \n",
    "        path=cat\n",
    "\n",
    "    with open(path) as jsonfile:\n",
    "        cat_members = json.load(jsonfile)\n",
    "        jsonfile.close()\n",
    "    for every in cat_members:\n",
    "        pages.append(every['title'])\n",
    "    \n",
    "print(\"Enter start date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2001-01-01\")\n",
    "input_date=input()\n",
    "if not input_date:\n",
    "    start_date=\"2001-01-01\"\n",
    "else:\n",
    "    start_date=input_date\n",
    "revisions=[]\n",
    "print(\"Starting harvest of revision history for \"+str(len(pages))+\" pages\")\n",
    "for page in pages:\n",
    "    Revisions=[]\n",
    "\n",
    "    print(\"Harvesting revision history for \"+page)    \n",
    "    URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": page,\n",
    "        \"rvlimit\": \"500\",\n",
    "        \"rvprop\": \"timestamp|user|comment|slotsize|userid|ids|tags\",\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"rvstart\": start_date+\"T00:00:00Z\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\"\n",
    "\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    if R.status_code==404:\n",
    "        print(\"The page does not exist\")\n",
    "    DATA = R.json()\n",
    "    for each in DATA['query']['pages']:\n",
    "        Revisions.append(each)\n",
    "\n",
    "    while 'continue' in DATA.keys():\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"revisions\",\n",
    "            \"titles\": page,\n",
    "            \"rvlimit\": \"500\",\n",
    "            \"rvprop\": \"timestamp|user|comment|slotsize|userid|ids|tags\",\n",
    "            \"rvdir\": \"newer\",\n",
    "            \"rvstart\": start_date+\"T00:00:00Z\",\n",
    "            \"formatversion\": \"2\",\n",
    "            \"format\": \"json\",\n",
    "            \"rvcontinue\": DATA['continue']['rvcontinue']\n",
    "\n",
    "        }\n",
    "\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        for each in DATA['query']['pages']:\n",
    "            Revisions.append(each)\n",
    "\n",
    "    for each in Revisions:\n",
    "        if \"revisions\" in each:\n",
    "            for every in each[\"revisions\"]:\n",
    "                if not \"user\" in every:\n",
    "                    every[\"user\"]=\"n/a\"\n",
    "                if not \"userid\" in every:\n",
    "                    every[\"userid\"]=\"n/a\"\n",
    "                if not \"comment\" in every:\n",
    "                    every[\"comment\"]=\"n/a\"\n",
    "                if not \"slotsize\" in every:\n",
    "                    every[\"slotsize\"]=\"n/a\"\n",
    "                if not \"tags\" in every:\n",
    "                    every[\"tags\"]=\"n/a\"\n",
    "                if not \"revid\" in every:\n",
    "                    every[\"revid\"]=\"n/a\"\n",
    "                if not \"parentid\" in every:\n",
    "                    every[\"parentid\"]=\"n/a\"\n",
    "                every[\"page\"]=page\n",
    "\n",
    "                revisions.append(every)\n",
    "filename='MC_script_7_'+'revisions_'+cat+'_from_'+start_date+'_to_now'\n",
    "json_path = filename+'_all.json'\n",
    "csv_path=filename+'_all.csv'\n",
    "\n",
    "for each in revisions: \n",
    "    user=each[\"user\"]\n",
    "    if re.match(r\"^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$\",user):\n",
    "        try:\n",
    "            geo=(reader.get(user))\n",
    "            if \"location\" in geo: \n",
    "                lat=geo[\"location\"][\"latitude\"]\n",
    "                long=geo[\"location\"][\"longitude\"]\n",
    "            else: \n",
    "                lat=\"\"\n",
    "                long=\"\"\n",
    "        except:\n",
    "            lat=\"\"\n",
    "            long=\"\"\n",
    "    else:\n",
    "        lat=\"\"\n",
    "        long=\"\"\n",
    "    each[\"latitude\"]=lat\n",
    "    each[\"longitude\"]=long\n",
    "    \n",
    "with open(json_path, 'w') as outfile:\n",
    "    json.dump(revisions, outfile)\n",
    "\n",
    "headers=['revision_id','parent_id', 'user_name', 'user_id', 'timestamp','size','comment', 'tags','latitude','longitude',\"page\"]\n",
    "csv_list=[headers]\n",
    "\n",
    "for each in revisions:\n",
    "    date=each['timestamp'].split('T')[0]\n",
    "\n",
    "    entry=[each[\"revid\"], each[\"parentid\"],each[\"user\"],each[\"userid\"],each[\"timestamp\"],each[\"size\"],each[\"comment\"],each[\"tags\"],each[\"latitude\"],each[\"longitude\"],each[\"page\"]]\n",
    "    csv_list.append(entry)\n",
    "\n",
    "with open(csv_path,\"w\", newline='',encoding='utf-8') as f:\n",
    "    wr = csv.writer(f, delimiter=\";\")\n",
    "    wr.writerows(csv_list)        \n",
    "locale=!pwd\n",
    "\n",
    "print(locale[0]+\"/\"+csv_path)\n",
    "print(locale[0]+\"/\"+json_path)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
