{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 3: Make two different networks based on in-text links to other wikipedia pages    \n",
    "\n",
    "Wikipedia uses different ways of referencing knowledge claims. The first is the system we used in tutorial 2 to build a co-reference network. In this tutorial we will use what we call in-text references. These references are all \"internal\" to wikipedia, meaning that they __only refer to other wikipedia articles.__ \n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549628832/Mapping%20controversies%202019/In-text_reference.jpg\" title=\"Category:circumcision\" style=\"width: 700px;\" /> \n",
    "By in-text we simply mean that __we only include those references that appear in the main text of an article.__ If you want to include ALL \"internal\" links, you can use the script presented in tutorial 8.  \n",
    "\n",
    "This script takes as input a JSON file with category members from Wikipedia (e.g. \"cat_members_circumcision_depth_2.json\") and builds two __directed__ networks. One network with the cat members (only) connected by in-text references and one with the cat-mebers + all the pages they point to connected by in-text references.\n",
    "\n",
    "The network will look somewhat like this in structure:\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549628658/Mapping%20controversies%202019/InTextRefNet.jpg\" title=\"Category:circumcision\" style=\"width: 800px;\" /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries\n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.\n",
    "\n",
    "\n",
    "__Obs: in this workbook we will be using the wikipedia and networkx libraries. If you have already installed them once, there is no need to do it again. You may simply skip to step 2.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia library has been imported\n",
      "NetworkX library has been imported\n"
     ]
    }
   ],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed \n",
    "\n",
    "import sys\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import wikipedia\n",
    "    print(\"Wikipedia library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"Wikipedia library not found. Installing...\")\n",
    "    !pip install wikipedia\n",
    "    try:#... and try to import it again\n",
    "        import wikipedia\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the wikipedia library. Please check your internet connection and consult output from the installation below\")\n",
    "try:\n",
    "    import networkx\n",
    "    print(\"NetworkX library has been imported\")\n",
    "except:\n",
    "    print(\"NetworkX library not found. Installing...\")\n",
    "    !pip install networkx\n",
    "    \n",
    "    try:\n",
    "        import networkx\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the NetworkX library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make the in-text links networks\n",
    "\n",
    "The next step is to make the network. Here, you need to input the path to the json files you got from the MCTutorial1_Wikipedia_HarvestCatMembers_final script. \n",
    "\n",
    "If the JSON files are in the same directory as the scripts, you only need to input relational directions (i.e. the name of the json file e.g. cat_members_circumcision_depth_2)\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1549444568/Mapping%20controversies%202019/Script_json_same_folder_in_text.jpg\" title=\"Folder\" style=\"width: 800px;\" /> \n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "cat_members_all=[]\n",
    "print(\"Enter the name of the category members json file you wish to use for keyword search (e.g.cat_members_circumcision_depth_2). If you have multiple files separate them with a comma\")\n",
    "filename= input()\n",
    "if \",\" in filename:\n",
    "\n",
    "    for each in filename.split(\",\"):\n",
    "\n",
    "\n",
    "        if not each.endswith(\".json\"):\n",
    "            path=each+\".json\"\n",
    "        else: \n",
    "            path=each\n",
    "            each=each.split(\".\")[0]\n",
    "        with open(path) as jsonfile:\n",
    "            cat_members = json.load(jsonfile)\n",
    "            jsonfile.close()\n",
    "        for every in cat_members:\n",
    "            cat_members_all.append(every)\n",
    "else:\n",
    "    print(\" \")\n",
    "\n",
    "\n",
    "    if not filename.endswith(\".json\"):\n",
    "        path=filename+\".json\"\n",
    "    else: \n",
    "        path=filename\n",
    "        filename=filename.split(\".\")[0]\n",
    "    with open(path) as jsonfile:\n",
    "        cat_members_all = json.load(jsonfile)\n",
    "        jsonfile.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "seen = []\n",
    "network = {}\n",
    "print(\"Harvesting in-text links from \"+str(len(cat_members_all))+\" wikipedia pages. This might take a while...\")\n",
    "print(\"\")\n",
    "count=1\n",
    "for each in cat_members_all:\n",
    "    title=each[\"title\"]\n",
    "    if count % 50 == 0:\n",
    "        print(\"In text links harvested from \"+str(count)+\" pages out of \"+str(len(cat_members_all))+\". Continuing harvest...\")\n",
    "    if not title in seen:\n",
    "        seen.append(title)\n",
    "        try:\n",
    "        \n",
    "            page = wikipedia.page(title)\n",
    "            \n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            print(\"Wikipedia thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all capitalized letters\")\n",
    "            try:\n",
    "                page = wikipedia.page(title.capitalize())\n",
    "                print(\"Success! \"+title+\" is no longer ambiguous\")\n",
    "            except wikipedia.exceptions.DisambiguationError:\n",
    "                print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Trying again with all lower letters\")\n",
    "                try:\n",
    "                    page = wikipedia.page(title.lower())\n",
    "                    print(\"Success! \"+title+\" is no longer ambiguous\")\n",
    "                except wikipedia.exceptions.DisambiguationError:\n",
    "                    print(\"Wikipedia still thinks \"+title+\" is ambiguous (returns several candidate pages). Skipping page...\")\n",
    "                    continue\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(\"The page \"+title+\" could not be found. Skipping page...\")\n",
    "            continue\n",
    "\n",
    "        except:\n",
    "            print(\"The page \"+title+\" failed due to unknown reason. Skipping...\")\n",
    "            print(\"\")\n",
    "            continue\n",
    "        html = page.html()\n",
    "        text_links = []\n",
    "        html = html.split('<p>')\n",
    "        for p in html[1:]:\n",
    "            p = p.split('</p>')[0]\n",
    "            links = p.split('<a href=\"')\n",
    "            for l in links[1:]:\n",
    "                if(' title=\"') in l:\n",
    "                    l = l.split(' title=\"')[1]\n",
    "                    l = l.split('\">')[0]\n",
    "                    l = l.replace(\"&#39;\",\"'\")\n",
    "                    text_links.append(l)\n",
    "        network.update({title:text_links})\n",
    "    count=count+1\n",
    "    \n",
    "print(\"All pages harvested...\")\n",
    "new_cat_members={}\n",
    "for each in cat_members_all:\n",
    "    new_cat_members[each[\"title\"]]={\"level\":each[\"level\"]}\n",
    "    \n",
    "membersonly_edges = []\n",
    "all_edges = []\n",
    "members = network.keys()\n",
    "print(\"Calculating networks...\")\n",
    "print(\"\")\n",
    "for source in network:\n",
    "    for target in network[source]:\n",
    "        edge = (source,target)\n",
    "        all_edges.append(edge)\n",
    "        if target in members:\n",
    "            membersonly_edges.append(edge)\n",
    "print(\"Saving networks...\")\n",
    "print(\"\")\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(membersonly_edges)\n",
    "nx.write_gexf(G, 'MCTutorial2_3_'+filename+'_InText_LinkNet_membersonly.gexf')\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(all_edges)\n",
    "for each in G.nodes:\n",
    "    if each in members:\n",
    "        G.nodes[each]['member_level'] = 'Level '+str(new_cat_members[each][\"level\"])\n",
    "    else:\n",
    "        G.nodes[each]['member_level'] = 'Not a member'\n",
    "nx.write_gexf(G, 'MCTutorial2_3_'+filename+'_InText_LinkNet_allpages.gexf')\n",
    "print(\"The script is done. You can find your network files by following these paths: \")\n",
    "print(\"\")\n",
    "locale=!pwd\n",
    "print(locale[0]+\"/\"+'MCTutorial2_3_'+filename+'_InText_LinkNet_membersonly.gexf')\n",
    "print(\"\")\n",
    "print(locale[0]+\"/\"+'MCTutorial2_3_'+filename+'_InText_LinkNet_allpages.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
