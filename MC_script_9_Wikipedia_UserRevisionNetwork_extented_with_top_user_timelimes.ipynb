{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies script 9 extended more: Users' revisions outside category members sphere\n",
    "This set of scripts will help you extend the user-revision networks to include all the revisions users make on pages outside the wikipedia category\n",
    "\n",
    "__BEFORE YOU START! Take a moment to draw what this actually mean.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries\n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.\n",
    "\n",
    "\n",
    "__Obs: in this workbook we will be using the requests library. If you have already installed it once, there is no need to do it again. You may simply skip to step 2.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed \n",
    "\n",
    "import sys\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import requests\n",
    "    print(\"Requests library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"Requests library not found. Installing...\")\n",
    "    !pip install requests\n",
    "    try:#... and try to import it again\n",
    "        import requests\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the requests library. Please check your internet connection and consult output from the installation below\")\n",
    "try:\n",
    "    import networkx\n",
    "    print(\"NetworkX library has been imported\")\n",
    "except:\n",
    "    print(\"NetworkX library not found. Installing...\")\n",
    "    !pip install networkx\n",
    "    \n",
    "    try:\n",
    "        import networkx\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the NetworkX library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make a user list based on revisions on pages in a wikipedia category json file or single wikipedia pages\n",
    "This step will make a list of all the users who make revisions on the pages you input. The list will be used later in the script\n",
    "\n",
    "In order to run the script, click on the cell below and press \"Run\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you want to input the pages to use to make the user list?\n",
      "Enter '1' if you want to use a category members json file.\n",
      "Enter '2' if you want to enter the pages manually.\n",
      "Enter '0' if you want to use category members json file AND enter pages manually.\n",
      "2\n",
      "Enter the names of the pages you wish to make a user list from. If multiple pages use comma separation (e.g. circumcision,Female genital mutilation etc)\n",
      "COVID-19 pandemic lockdowns\n",
      "Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):\n",
      "en\n",
      "Enter start date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2001-01-01\n",
      "\n",
      "Enter end date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2020-03-01\n",
      "\n",
      "Enter user names you want to blacklist. If you want to blacklist multiple users, use comma separation.\n",
      "\n",
      "Harvesting data from 1 input pages...\n",
      "Done harvesting users\n",
      "There are 0 unique users making revisions in the category/page\n",
      "The users are stored in memory, and you may continue to step 3 or 4\n",
      "Dumping JSON file with all users. \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import networkx as nx\n",
    "filename=\"\"\n",
    "print(\"How do you want to input the pages to use to make the user list?\")\n",
    "print(\"Enter '1' if you want to use a category members json file.\")\n",
    "print(\"Enter '2' if you want to enter the pages manually.\")\n",
    "print(\"Enter '0' if you want to use category members json file AND enter pages manually.\")\n",
    "pages=[]\n",
    "input_style=input()\n",
    "if input_style==str(1) or input_style==1 or input_style==0 or input_style==str(0):\n",
    "    print(\"Enter the name of the category members json file you wish to make a user list from (e.g.cat_members_circumcision_depth_2). If you have multiple files separate them with a comma\")\n",
    "    filename= input()\n",
    "    if \",\" in filename:\n",
    "        \n",
    "        for each in filename.split(\",\"):\n",
    "            \n",
    "\n",
    "            if not each.endswith(\".json\"):\n",
    "                path=each+\".json\"\n",
    "            else: \n",
    "                path=each\n",
    "                each=each.split(\".\")[0]\n",
    "            with open(path) as jsonfile:\n",
    "                cat_members = json.load(jsonfile)\n",
    "                jsonfile.close()\n",
    "            for every in cat_members:\n",
    "                pages.append(every['title'])\n",
    "    else:\n",
    "        print(\" \")\n",
    "        \n",
    "\n",
    "        if not filename.endswith(\".json\"):\n",
    "            path=filename+\".json\"\n",
    "        else: \n",
    "            path=filename\n",
    "            filename=filename.split(\".\")[0]\n",
    "        with open(path) as jsonfile:\n",
    "            cat_members = json.load(jsonfile)\n",
    "            jsonfile.close()\n",
    "        for each in cat_members:\n",
    "            pages.append(each['title'])\n",
    "    \n",
    "if input_style==str(2) or input_style==2 or input_style==0 or input_style==str(0):\n",
    "    print(\"Enter the names of the pages you wish to make a user list from. If multiple pages use comma separation (e.g. circumcision,Female genital mutilation etc)\")\n",
    "    raw_input=input()\n",
    "    \n",
    "    if \",\" in raw_input:\n",
    "        for each in raw_input.split(\",\"):\n",
    "            pages.append(each)\n",
    "    else:\n",
    "        pages.append(raw_input)\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "\n",
    "print(\"Enter start date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2001-01-01\")\n",
    "start_date=input()\n",
    "if not start_date:\n",
    "    start_date=\"2001-01-01\"\n",
    "print(\"Enter end date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2020-03-01\")\n",
    "end_date=input()\n",
    "if not end_date:\n",
    "    end_date=\"2020-03-01\"\n",
    "blacklist=[]\n",
    "blacklisted_users=[]\n",
    "\n",
    "print(\"Enter user names you want to blacklist. If you want to blacklist multiple users, use comma separation.\")\n",
    "raw_blacklist=input()\n",
    "raw_blacklist=\"\"\n",
    "if \",\" in raw_blacklist:\n",
    "    for each in raw_blacklist.split(\",\"):\n",
    "        blacklisted_users.append(each.strip())\n",
    "else: \n",
    "    blacklisted_users.append(raw_blacklist.strip())\n",
    "\n",
    "print(\"Harvesting data from \"+str(len(pages))+\" input pages...\")\n",
    "count=1\n",
    "users=[]\n",
    "user_dict={}\n",
    "for page in pages:\n",
    "    Revisions = []\n",
    "    URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
    "    if count % 50 == 0:\n",
    "        print(\"Data harvested from \"+str(count)+\" pages out of \"+str(len(pages))+\". Continuing harvest...\")\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": page,\n",
    "        \"rvlimit\": \"500\",\n",
    "        \"rvprop\": \"user|timestamp\",\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"rvstart\": start_date+\"T00:00:00Z\",\n",
    "        \"rvend\": end_date+\"T00:00:00Z\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"format\": \"json\"\n",
    "\n",
    "    }\n",
    "\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    if R.status_code==404:\n",
    "        print(\"The page does not exist. Skipping...\")\n",
    "        continue\n",
    "    DATA = R.json()\n",
    "    for each in DATA['query']['pages']:\n",
    "        Revisions.append(each)\n",
    "\n",
    "    while 'continue' in DATA.keys():\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"revisions\",\n",
    "            \"titles\": page,\n",
    "            \"rvlimit\": \"500\",\n",
    "            \"rvprop\": \"user|timestamp\",\n",
    "            \"rvdir\": \"newer\",\n",
    "            \"rvstart\": start_date+\"T00:00:00Z\",\n",
    "            \"rvend\": end_date+\"T00:00:00Z\",\n",
    "            \"formatversion\": \"2\",\n",
    "            \"format\": \"json\",\n",
    "            \"rvcontinue\": DATA['continue']['rvcontinue']\n",
    "\n",
    "        }\n",
    "\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        for each in DATA['query']['pages']:\n",
    "            Revisions.append(each)\n",
    "\n",
    "\n",
    "    for each in Revisions:\n",
    "        if \"revisions\" in each:\n",
    "            for every in each[\"revisions\"]:\n",
    "                if \"user\" in every:\n",
    "                    if every[\"user\"] not in blacklisted_users:\n",
    "                        users.append(every[\"user\"])\n",
    "                        if every[\"user\"] in user_dict:\n",
    "                            user_dict[every[\"user\"]][\"timestamps\"].append(every[\"timestamp\"])\n",
    "\n",
    "                        else:\n",
    "                            user_dict[every[\"user\"]]={\"timestamps\":[every[\"timestamp\"]]}\n",
    "    count=count+1\n",
    "print(\"Done harvesting users\")\n",
    "\n",
    "print(\"There are \"+str(len(set(users)))+\" unique users making revisions in the category/page\")\n",
    "print(\"The users are stored in memory, and you may continue to step 3 or 4\")\n",
    "print(\"Dumping JSON file with all users. \")\n",
    "filename_json=\"users_making_revisions_\"+start_date+\"_to_\"+end_date+\".json\"\n",
    "\n",
    "with open(filename_json, 'w') as outfile:\n",
    "    json.dump(users, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export timeline information on top active users\n",
    "This script will output a csv file with the top n users counted on revision history, in order to make a simple timeline in tableau or a spreadsheet editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many top users do you wan't to include?\n",
      "10\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import datetime\n",
    "cnt=Counter(users)\n",
    "print(\"How many top users do you wan't to include?\")\n",
    "top=int(input())\n",
    "\n",
    "start_year=int(start_date.split(\"-\")[0])\n",
    "end_year=int(end_date.split(\"-\")[0])\n",
    "years=[]\n",
    "for each in range(start_year,end_year+1):\n",
    "    years.append(str(each))\n",
    "    \n",
    "months=['01', '02','03','04','05','06','07','08','09','10','11','12']\n",
    "top_users=[]\n",
    "for each in cnt.most_common(top):\n",
    "    top_users.append(each[0])\n",
    "\n",
    "dict_of_years={}\n",
    "csv_path_count='top_'+str(top)+'_user_revisions_count_month.csv'\n",
    "headers=[\"user\",\"Time period (months)\", \"Revision count\"]\n",
    "\n",
    "csv_total=[headers]\n",
    "for user in top_users:\n",
    "    dict_of_years[user]={\"years\":{}}\n",
    "    for year in years:\n",
    "        dict_of_years[user][\"years\"][year]={}\n",
    "        if not year==\"2020\":\n",
    "            for month in months:\n",
    "                dict_of_years[user][\"years\"][year][month]={\"count\":0}\n",
    "        else: \n",
    "            for month in months[:int(str(datetime.datetime.now()).split(\"-\")[1])]:\n",
    "                dict_of_years[user][\"years\"][year][month]={\"count\":0}\n",
    "\n",
    "for user in top_users:\n",
    "    timestamps=user_dict[user][\"timestamps\"]\n",
    "    for timestamp in timestamps:\n",
    "        user_id=user\n",
    "        year=timestamp.split('-')[0]\n",
    "        month=timestamp.split('-')[1]\n",
    "        dict_of_years[user][\"years\"][year][month][\"count\"]=dict_of_years[user][\"years\"][year][month][\"count\"]+1\n",
    "for user in dict_of_years:\n",
    "    for year in dict_of_years[user][\"years\"]:\n",
    "        for month in dict_of_years[user][\"years\"][year]:\n",
    "            count=dict_of_years[user][\"years\"][year][month][\"count\"]\n",
    "            period=str(year)+\"-\"+str(month)\n",
    "            csv_list=[user,period,count]\n",
    "            csv_total.append(csv_list)\n",
    "with open(csv_path_count,\"w\", newline='',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=\";\")\n",
    "    writer.writerows(csv_total)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Harvest ALL revisions from the users on the user list\n",
    "\n",
    "This script will harvest all revisions made by the users, who make revisions within the category, categories or pages you inputted in step 2. \n",
    "\n",
    "__Be aware, that including many users in combination with a wide date range might increase the time it takes to run the script dramastically__ \n",
    "\n",
    "It is impossible to say how long a harvest will take, as we dont know in advance how many revisions users make. But as a rule of thumb, if you wan't to harvest activity from all the users, you should limit the date range to a week or so, and evaluate how long it takes. You can then increase the date range to e.g. a month. __You will be able to set a net date range below__.\n",
    "Alternatively, the script allows you to filter out users from the list based on their activity (high/low), and make a randomized sample. \n",
    "\n",
    "It might not always be the most active users, that are the most interesting. What do you think characterises the most active users? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"Before you start, please read the text above!\")\n",
    "\n",
    "print('Enter the desired language version of wikipedia (e.g. \"en\",\"da\",\"fr\",etc.) or leave blank to use default (english):')\n",
    "\n",
    "input_lan = input()\n",
    "if not input_lan:\n",
    "    lan=\"en\"\n",
    "else:\n",
    "    lan=input_lan\n",
    "\n",
    "print(\"Enter user names you want to blacklist. If you want to blacklist multiple users, use comma separation.\")\n",
    "raw_blacklist=\"\"\n",
    "raw_blacklist=input()\n",
    "\n",
    "blacklisted_users=[]\n",
    "if \",\" in raw_blacklist:\n",
    "    for each in raw_blacklist.split(\",\"):\n",
    "        blacklisted_users.append(each.strip().lower())\n",
    "else: \n",
    "    blacklisted_users.append(raw_blacklist.strip().lower())\n",
    "\n",
    "if 'users' in locals():\n",
    "    print(\"The user list currently include \"+str(len(set(users)))+\" unique users.\")\n",
    "\n",
    "else: \n",
    "    sys.exit(\"Go to step 2\")\n",
    "    \n",
    "\n",
    "\n",
    "new_users=[]\n",
    "\n",
    "print(\"Do you wan't to exclude users that has 'bot' in its name (y/n)?\")\n",
    "bot=input()\n",
    "if bot==\"y\":\n",
    "    for user in list(set(users)):\n",
    "        if \"bot\" not in user.lower():\n",
    "            new_users.append(user)\n",
    "    print(\"The user list now contains \"+str(len(new_users))+\" users\")\n",
    "users=new_users\n",
    "new_users=[]\n",
    "print(\"Would you like to filter the users (y/n)? (Most likely yes!)\")\n",
    "\n",
    "filter_users=input()             \n",
    "if filter_users.lower()==\"y\":\n",
    "    print(\"Select how you wan't to filter the users\")\n",
    "    print(\"Enter '1' if you ONLY wan't to include the top n most active users (where 'n' will be set later)\")\n",
    "    print(\"Enter '2' if you ONLY wan't to include the n least active users (where 'n' will be set later)\")\n",
    "    print(\"Enter '3' if you wan't to make a randomized sample of n users (where 'n' will be set later)\")\n",
    "    choice=input()\n",
    "    print(\"Enter the number of users you wan't to include\")\n",
    "    limit=input()\n",
    "    if limit:\n",
    "        limit=int(limit)\n",
    "        if limit>len(set(users)):\n",
    "            limit=len(set(users))\n",
    "    else:\n",
    "        limit=len(set(users))\n",
    "\n",
    "else:\n",
    "    choice=\"\"\n",
    "    for user in list(set(users)):\n",
    "        if user.lower() not in blacklisted_users:\n",
    "            new_users.append(user)\n",
    "print(\"Enter a limit for how many revisions you would like to include per user (will round up to nearest 500 per user). Leave blank for no limit:\")\n",
    "user_limit=input()\n",
    "if user_limit:\n",
    "    user_limit=int(user_limit)\n",
    "else:\n",
    "    user_limit=100000000\n",
    "\n",
    "\n",
    "print(\"Enter start date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2001-01-01\")\n",
    "start_date=input()\n",
    "if not start_date:\n",
    "    start_date=\"2001-01-01\"\n",
    "print(\"Enter end date for revisions in the format: yyyy-mm-dd. Leave blank to use default date: 2020-03-01\")\n",
    "end_date=input()\n",
    "if not end_date:\n",
    "    end_date=\"2020-03-01\"\n",
    "\n",
    "cnt=Counter(users)\n",
    "#len(cnt)\n",
    "\n",
    "print(\"The script will make a folder to which the exports will be saved. What would you like to call it? Leave blank for default 'json dumps user revisions'.\")\n",
    "print(\"IMPORTANT NOTE: if you have run the script before, DO NOT use the same folder, as the script will overwrite the content!!\")\n",
    "\n",
    "\n",
    "K=True\n",
    "while K:\n",
    "    #raw_dump_path=input()\n",
    "    raw_dump_path=input()\n",
    "    if not raw_dump_path:\n",
    "        raw_dump_path=\"json dumps user revisions\"\n",
    "    dump_path=raw_dump_path+\"/\"\n",
    "    try:\n",
    "        os.mkdir(dump_path)\n",
    "        K=False\n",
    "    except FileExistsError:\n",
    "        print(\"The folder already exists. Please enter another name\")\n",
    "        \n",
    "\n",
    "ranged_cnt=cnt.most_common(len(cnt))\n",
    "#ranged_cnt\n",
    "if choice==\"1\":\n",
    "    for each in ranged_cnt[:limit]:\n",
    "        if each[0].lower() not in blacklisted_users:\n",
    "                \n",
    "            new_users.append(each[0])\n",
    "if choice==\"2\":\n",
    "    for each in ranged_cnt[len(cnt)-limit:]:\n",
    "        if each[0].lower() not in blacklisted_users:\n",
    "            new_users.append(each[0])\n",
    "if choice==\"3\":\n",
    "    random_indexes=[]\n",
    "    for x in range(limit):\n",
    "        random_indexes.append(random.randint(0,len(cnt)-1))\n",
    "    for random_index in random_indexes:\n",
    "        if ranged_cnt[random_index][0].lower() not in blacklisted_users:\n",
    "            new_users.append(ranged_cnt[random_index][0])\n",
    "\n",
    "    \n",
    "S = requests.Session()\n",
    "\n",
    "\n",
    "print(\"Harvesting data from \"+str(len(new_users))+\" users\")\n",
    "total_rev_counts=0\n",
    "dumpcount=1\n",
    "for user in new_users:\n",
    "    if new_users.index(user) % 1000 == 0 and new_users.index(user)!=0:\n",
    "        print(\"The script has harvested \"+str(total_rev_counts)+\" revisions from \"+str(new_users.index(user))+\" users.\")\n",
    "\n",
    "        json_dump_name=dump_path+\"json_dump_\"+str(dumpcount)+\".json\"\n",
    "\n",
    "        with open(json_dump_name, 'w') as outfile:\n",
    "            json.dump(user_dict, outfile)\n",
    "        dumpcount=dumpcount+1\n",
    "        user_dict={}\n",
    "    if user.lower() not in blacklisted_users:\n",
    "        user_count=0\n",
    "\n",
    "        user_dict[user]={}\n",
    "        URL = \"http://\"+lan+\".wikipedia.org/w/api.php\"\n",
    "\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"allrevisions\",\n",
    "\n",
    "            \"arvlimit\": \"500\",\n",
    "            \"arvprop\": \"user|ids|comment|timestamp|size\",\n",
    "            \"arvdir\": \"newer\",\n",
    "            \"arvstart\": start_date+\"T00:00:00Z\",\n",
    "            \"arvend\": end_date+\"T00:00:00Z\",\n",
    "            \"arvuser\":user,\n",
    "            \"formatversion\": \"2\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        try:\n",
    "            R = S.get(url=URL, params=PARAMS)\n",
    "            DATA = R.json()\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(5)\n",
    "                R = S.get(url=URL, params=PARAMS)\n",
    "                DATA = R.json()\n",
    "            except:\n",
    "                sys.exit(\"Something went wrong. Exiting...\")\n",
    "        if DATA[\"query\"][\"allrevisions\"]:\n",
    "            for page in DATA['query']['allrevisions']:\n",
    "                page_id=page[\"pageid\"]\n",
    "                page_title=page[\"title\"]\n",
    "\n",
    "                if \"talk\" in page_title.lower():\n",
    "                    page_title=page_title.lower().split(\":\")[len(page_title.lower().split(\":\"))-1]\n",
    "\n",
    "                if page_title.lower() in pages:\n",
    "                    in_cat=\"yes\"\n",
    "                else:\n",
    "                    in_cat=\"no\"\n",
    "                page_revisions=[]\n",
    "                for revision in page[\"revisions\"]:\n",
    "                    page_revisions.append(revision)\n",
    "                    total_rev_counts=total_rev_counts+1\n",
    "                if not page_id in user_dict[user]:\n",
    "                    \n",
    "                    user_dict[user][page_id]={\"page_title\":page_title, \"is_page_in_cat\":in_cat,\"page_revisions\":page_revisions}\n",
    "                    user_count=user_count+1\n",
    "                else:\n",
    "                    for revision in page_revisions:\n",
    "                        \n",
    "                        user_dict[user][page_id][\"page_revisions\"].append(revision)\n",
    "                        user_count=user_count+1\n",
    "        if user_count>int(user_limit):\n",
    "            continue\n",
    "\n",
    "        while 'continue' in DATA.keys() and user_count<user_limit:\n",
    "            PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"allrevisions\",\n",
    "\n",
    "            \"arvlimit\": \"500\",\n",
    "            \"arvprop\": \"user|ids|comment|timestamp|size\",\n",
    "            \"arvdir\": \"newer\",\n",
    "            \"arvstart\": start_date+\"T00:00:00Z\",\n",
    "            \"arvend\": end_date+\"T00:00:00Z\",\n",
    "            \"arvuser\":user,\n",
    "            \"formatversion\": \"2\",\n",
    "            \"format\": \"json\",\n",
    "            \"arvcontinue\": DATA['continue']['arvcontinue']\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                R = S.get(url=URL, params=PARAMS)\n",
    "                DATA = R.json()\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(5)\n",
    "                    R = S.get(url=URL, params=PARAMS)\n",
    "                    DATA = R.json()\n",
    "                except:\n",
    "                    sys.exit(\"Something went wrong. Exiting...\")                    \n",
    "            if DATA[\"query\"][\"allrevisions\"]:\n",
    "                for page in DATA['query']['allrevisions']:\n",
    "                    page_id=page[\"pageid\"]\n",
    "                    page_title=page[\"title\"]\n",
    "                    if \"talk\" in page_title.lower():\n",
    "                        page_title=page_title.lower().split(\":\")[len(page_title.lower().split(\":\"))-1]\n",
    "\n",
    "                    for cat_page in pages:\n",
    "                        if page_title.lower() == cat_page.lower():\n",
    "                            in_cat=\"yes\"\n",
    "                        else:\n",
    "                            in_cat=\"no\"\n",
    "                    page_revisions=[]\n",
    "                    for revision in page[\"revisions\"]:\n",
    "                        page_revisions.append(revision)\n",
    "                        total_rev_counts=total_rev_counts+1\n",
    "                    if not page_id in user_dict[user]:\n",
    "                        \n",
    "                        user_dict[user][page_id]={\"page_title\":page_title,\"is_page_in_cat\":in_cat, \"page_revisions\":page_revisions}\n",
    "                        user_count=user_count+1\n",
    "                    else:\n",
    "                        for revision in page_revisions:\n",
    "                            \n",
    "                            user_dict[user][page_id][\"page_revisions\"].append(revision)\n",
    "                            user_count=user_count+1\n",
    "\n",
    "json_dump_name=dump_path+\"json_dump_\"+str(dumpcount)+\".json\"\n",
    "\n",
    "with open(json_dump_name, 'w') as outfile:\n",
    "    json.dump(user_dict, outfile)\n",
    "\n",
    "print(\"You have harvested \"+str(total_rev_counts)+\" revisions. \")\n",
    "\n",
    "print(\"The script is done! Go to step 5 to export networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Built bi partite network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built bi-partite network\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "\n",
    "if \"dump_path\" in locals():\n",
    "    \n",
    "    if os.path.exists(dump_path):\n",
    "        print(\"The script has identified this folder containing json files: \"+dump_path)\n",
    "\n",
    "        print(\"If it is not correct, please enter the correct foldername below. If it is correct, leave it blank\")\n",
    "        hg=input()\n",
    "        if hg:\n",
    "            dump_path=hg+\"/\"\n",
    "    else:\n",
    "        print(\"The script could not identify the folder containing the json files from step 3 \")\n",
    "    \n",
    "        print(\"Please enter the correct foldername below\")\n",
    "        hg=input()\n",
    "        if hg:\n",
    "            dump_path=hg+\"/\"\n",
    "else:\n",
    "    print(\"The script could not identify the folder containing the json files from step 3 \")\n",
    "    \n",
    "    print(\"Please enter the correct foldername below\")\n",
    "    hg=input()\n",
    "    if hg:\n",
    "        dump_path=hg+\"/\"\n",
    "\n",
    "print(\"Do you wan't to introduce an edge filter (y/n)?\")\n",
    "filter_c=input()\n",
    "if filter_c.lower()==\"y\":\n",
    "    print(\"Enter the minimum weight between a user and a page (leave blank for no filter): \")\n",
    "    edge_filter_low=input()\n",
    "    if not edge_filter_low:\n",
    "        edge_filter_low=0\n",
    "    print(\"Enter the maximum weight between a user and a page (leave blank for no filter): \")\n",
    "    edge_filter_high=input()\n",
    "    if not edge_filter_high:\n",
    "        edge_filter_high=1000000000\n",
    "        \n",
    "print(\"Do you wan't to filter out Wikipedia About pages (y/n)? (e.g. https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Noticeboard)\")\n",
    "filter_about=input()\n",
    "print(\"What would you like to call the network?\")\n",
    "name=input()\n",
    "if name:\n",
    "    name=name+\"_\"\n",
    "else: \n",
    "    name=\"\"\n",
    "\n",
    "G = nx.Graph()\n",
    "edges=[]\n",
    "users_=[]\n",
    "print(\"Generating network...\")\n",
    "filenames=os.listdir(dump_path)\n",
    "for filename in filenames:\n",
    "    path_to_json=dump_path+filename\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(path_to_json) as jsonfile:\n",
    "            user_dict = json.load(jsonfile)\n",
    "            jsonfile.close()\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    for user in user_dict:\n",
    "        users_.append(user)\n",
    "    for user in user_dict:\n",
    "        for page in user_dict[user]:\n",
    "            if filter_about.lower()==\"y\":    \n",
    "                page_title=user_dict[user][page][\"page_title\"]\n",
    "                if \"wikipedia:\" in page_title.lower():\n",
    "                    continue\n",
    "            weight=len(user_dict[user][page][\"page_revisions\"])\n",
    "            if weight<=int(edge_filter_low):\n",
    "                continue\n",
    "            if weight>=int(edge_filter_high):\n",
    "                continue\n",
    "            edges=[]\n",
    "            edge = (user,page,{\"weight\":weight})\n",
    "            edges.append(edge)\n",
    "            G.add_edges_from(edges)\n",
    "            G.nodes[page][\"is_page_in_cat\"]=user_dict[user][page][\"is_page_in_cat\"]\n",
    "            G.nodes[page][\"page_title\"]=user_dict[user][page][\"page_title\"]\n",
    "            G.nodes[page][\"label\"]=user_dict[user][page][\"page_title\"]\n",
    "            G.nodes[page][\"type\"]=\"page\"\n",
    "            G.nodes[user][\"type\"]=\"user\"\n",
    "            G.nodes[user][\"label\"]=user\n",
    "            G.nodes[user][\"is_page_in_cat\"]=\"N/A\"\n",
    "\n",
    "\n",
    "nx.write_gexf(G, name+'user2page_revision_bipartite.gexf')\n",
    "print('Bipartite network saved. You can find the network by following this path: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
