{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping controversies tutorial x: harvest submissions and comments from Subreddits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the right libraries \n",
    "Libraries for Jupyter can be understood as preprogrammed script parts. This means, that instead of writing a lot of lines of code in order e.g. make contact to Wikipedia, you can do it in one command.  \n",
    "\n",
    "In order to run the installation, click on the cell below and press \"Run\" in the menu. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell Jupyter checks whether you have the right libraries installed to carry out the harvest of data from Wikipedia\n",
    "\n",
    "try: #First, Jupyter tries to import a library\n",
    "    import praw\n",
    "    print(\"praw library has been imported\")\n",
    "except: #If it fails, it will try to install the library\n",
    "    print(\"praw library not found. Installing...\")\n",
    "    !pip install praw\n",
    "    try:#... and try to import it again\n",
    "        import praw\n",
    "    except: #unless it fails, and raises an error.\n",
    "        print(\"Something went wrong in the installation of the praw library. Please check your internet connection and consult output from the installation below\")\n",
    "try:\n",
    "    import pandas\n",
    "    print(\"pandas api library has been imported\")\n",
    "except:\n",
    "    print(\"pandas api library not found. Installing...\")\n",
    "    !pip install pandas\n",
    "    \n",
    "    try:\n",
    "        import pandas\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the pandas api library. Please check your internet connection and consult output from the installation below\")\n",
    "\n",
    "try:\n",
    "    import psaw\n",
    "    print(\"psaw api library has been imported\")\n",
    "except:\n",
    "    print(\"psaw api library not found. Installing...\")\n",
    "    !pip install psaw\n",
    "    \n",
    "    try:\n",
    "        import psaw\n",
    "    except:\n",
    "        print(\"Something went wrong in the installation of the psaw api library. Please check your internet connection and consult output from the installation below\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Reddit app\n",
    "\n",
    "The first step is to create an app for reddit. This is done in order to get access to the API. You can do so by following the first step of [this tutorial](http://www.storybench.org/how-to-scrape-reddit-with-python/). \n",
    "\n",
    "### When you have the _14-characters personal use script_ and _27-character secret key_  run the cell below and input the information.\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dra3btd6p/image/upload/v1550562228/Mapping%20controversies%202019/reddit_app.jpg\" title=\"Category:circumcision\" style=\"width: 900px;\" /> \n",
    "\n",
    "\n",
    "### You need to run the cell below and input the information before you go to step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the 27 character secret key from the app page: \n"
     ]
    }
   ],
   "source": [
    "##### RUN THIS CELL FIRST!\n",
    "\n",
    "\n",
    "print(\"Enter the 27 character secret key from the app page: \")\n",
    "secret=input()\n",
    "\n",
    "print(\"Enter the 14 character personal use script key from the app page: \")\n",
    "pus=input()\n",
    "\n",
    "print(\"Enter your app-name :\")\n",
    "app_name=input()\n",
    "\n",
    "print(\"Enter your reddit user name: \")\n",
    "user_name=input()\n",
    "\n",
    "print(\"Enter your reddit password: \")\n",
    "pw=input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Harvest the data from Reddit\n",
    "\n",
    "The Reddit API does not allow for a complete harvest of all data. You are allowed to harvest a maximum of 1000 submissions from a subreddit (e.g. www.reddit.com/r/askreddit), but from those 1000 submissions you can harvest all comments and replies. The 1000 submissions can be selected based on four different measures:\n",
    "\n",
    "- Hottest\n",
    "- Controversial\n",
    "- Newest\n",
    "- Top\n",
    "- Date range\n",
    "\n",
    "(https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html?highlight=submissions#praw.models.Subreddit.submissions)\n",
    "\n",
    "The script outputs entries (submissions and comments) in a csv file (one entry per line). You can then use Table2Net or other tools to make sense of the data. Comments will refer to submissions in the row \"parent_id\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to call the file?\n",
      "\n",
      "Enter user names you want to blacklist. If you want to blacklist multiple users, use comma separation\n",
      "\n",
      "Would you like to enter the submission id's manually (y/n)? Max 1000 submissions. Use this feature if you have manually identified submissions to focus on.\n",
      "n\n",
      "Enter the name of the subreddit you would like to harvest: (e.g. askreddit)\n",
      "geek\n",
      "How many submissions would you like to harvest? (Be carefull! It will take time to harvest 1000 submissions with a lot of comments!)\n",
      "10\n",
      "What type of measure would you like to use to define how the submissions are selected?\n",
      "Press '1' for the 10 'hottest' submissions\n",
      "Press '2' for the 10 most 'controversial' submissions (be aware of what counts as controversial for reddit might not be in line with our definition)\n",
      "Press '3' for the 10 'newest' submissions\n",
      "Press '4' for the 10 'top' submissions\n",
      "Press '5' to set a start date\n",
      "2\n",
      "Harvesting data... This might take a while...\n",
      "The script is done! We have harvested 10 submissions with 1055 comments in total.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import praw\n",
    "import psaw\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "\n",
    "    \n",
    "reddit = praw.Reddit(client_id=pus, \\\n",
    "                     client_secret=secret, \\\n",
    "                     user_agent=app_name, \\\n",
    "                     username=user_name, \\\n",
    "                     password=pw)\n",
    "api=psaw.PushshiftAPI(reddit)\n",
    "\n",
    "print(\"What would you like to call the file?\")\n",
    "input_filename=input()\n",
    "measure=\"0\"\n",
    "\n",
    "csv_headers=[\"Type\",\"id\",\"author\",\"body\",\"created\",\"up_votes\",\"down_votes\",\"likes\",\"depth\", \"parent_id\", \"url\", \"reports\", \"subreddit\",\"submission_id\"]\n",
    "com_count=0\n",
    "sub_count=0\n",
    "csv_headers_2=[\"submission_id\", \"submission_url\", \"users\", \"subreddit\"]\n",
    "\n",
    "\n",
    "blacklisted_users=[]\n",
    "print(\"Enter user names you want to blacklist. If you want to blacklist multiple users, use comma separation\")\n",
    "raw_blacklist=input()\n",
    "raw_blacklist=\"\"\n",
    "if \",\" in raw_blacklist:\n",
    "    for each in raw_blacklist.split(\",\"):\n",
    "        blacklisted_users.append(each.strip().lower())\n",
    "else: \n",
    "    blacklisted_users.append(raw_blacklist.strip().lower())\n",
    "\n",
    "print(\"Would you like to enter the submission id's manually (y/n)? Max 1000 submissions. Use this feature if you have manually identified submissions to focus on.\")\n",
    "manual_sub=input()\n",
    "if manual_sub.lower()==\"y\":\n",
    "    submissions=[]\n",
    "    print(\"Enter the id of the submission(s). If more, separate by comma.\")\n",
    "    manual_list=input()\n",
    "    for each in manual_list.split(\",\"):\n",
    "        each=each.replace(\" \",\"\")\n",
    "        submissions.append(each)\n",
    "    print(\"Harvesting data... This might take a while...\")\n",
    "\n",
    "    if \".csv\" in input_filename:\n",
    "        filename=\"submissions_\"+input_filename\n",
    "        filename_2=\"user_aggregates_submissions_\"+input_filename\n",
    "    else:\n",
    "        filename=\"submissions_\"+input_filename+\".csv\"\n",
    "        filename_2=\"user_aggregates_submissions_\"+input_filename+\".csv\"\n",
    "\n",
    "    with open(filename,\"w\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_headers)\n",
    "    with open(filename_2,\"w\", newline='',encoding='utf-8') as q:\n",
    "        wr2 = csv.writer(q, delimiter=\",\")\n",
    "        wr2.writerow(csv_headers_2)\n",
    "    for each in submissions:\n",
    "        sub=reddit.submission(str(each))\n",
    "        sub_author=sub.author\n",
    "        sub_downs=sub.downs\n",
    "        sub_ups=sub.ups\n",
    "        sub_likes=sub.likes\n",
    "        sub_title=sub.title\n",
    "        sub_created=sub.created\n",
    "        sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        subreddit_name=str(sub.subreddit)\n",
    "        sub_id=sub.id\n",
    "        sub_reports=sub.num_reports\n",
    "        sub_text=sub.selftext\n",
    "        sub_users=[]\n",
    "        sub_users.append(str(sub_author))\n",
    "        sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        sub_list=sub.comments.list()\n",
    "        sub_count=sub_count+1\n",
    "        csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports,subreddit_name, sub_id]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        for comment in sub_list:\n",
    "            comment_depth=comment.depth\n",
    "            comment_parent_id=comment.parent_id\n",
    "            comment_parent_id=comment_parent_id.split(\"_\")[1]\n",
    "            comment_reports=comment.num_reports\n",
    "            comment_author=str(comment.author)\n",
    "            if comment_author.lower() in blacklisted_users:\n",
    "                continue\n",
    "            comment_body=comment.body\n",
    "            comment_created=comment.created_utc\n",
    "            comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comment_downs=comment.downs\n",
    "            comment_ups=comment.ups\n",
    "            comment_likes=comment.likes\n",
    "            comment_id=comment.id\n",
    "            sub_users.append(str(comment_author))\n",
    "            comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "            csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports,subreddit_name, sub_id]\n",
    "            with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "                wr = csv.writer(f, delimiter=\",\")\n",
    "                wr.writerow(csv_list)\n",
    "            com_count=com_count+1\n",
    "        user_entry=\"\"\n",
    "        for user in sub_users:\n",
    "            if not sub_users.index(user)==len(sub_users)-1:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user+\";\"\n",
    "            else:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user\n",
    "        csv_list_2=[str(sub_id), str(sub_url), user_entry,str(subreddit_name)]\n",
    "        with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "            wr2 = csv.writer(q, delimiter=\",\")\n",
    "            wr2.writerow(csv_list_2)\n",
    "        if sub_count % 50 == 0:\n",
    "            print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum \"+str(max_response_cache)+\". Continuing harvest...\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    print(\"Enter the name of the subreddit you would like to harvest: (e.g. askreddit)\")\n",
    "    subreddit_name=input()\n",
    "    print(\"How many submissions would you like to harvest? (Be carefull! It will take time to harvest 1000 submissions with a lot of comments!)\")\n",
    "    sub_limit=input()\n",
    "    sub_limit=int(sub_limit)\n",
    "    print(\"What type of measure would you like to use to define how the submissions are selected?\")\n",
    "    max_response_cache=sub_limit\n",
    "    print(\"Press '1' for the \"+str(sub_limit)+\" 'hottest' submissions\")\n",
    "    print(\"Press '2' for the \"+str(sub_limit)+\" most 'controversial' submissions (be aware of what counts as controversial for reddit might not be in line with our definition)\")\n",
    "    print(\"Press '3' for the \"+str(sub_limit)+\" 'newest' submissions\")\n",
    "    print(\"Press '4' for the \"+str(sub_limit)+\" 'top' submissions\")\n",
    "    print(\"Press '5' to set a start date\")\n",
    "    measure=input()\n",
    "    if measure==\"5\":\n",
    "        print(\"Set the start date: (Format: yyyy-mm-dd. The script will harvest the first \"+str(sub_limit)+ \" submissions made from the start date )\")\n",
    "        start_date=input()\n",
    "        start_year=int(start_date.split(\"-\")[0])\n",
    "        start_month=int(start_date.split(\"-\")[1])\n",
    "        start_day=int(start_date.split(\"-\")[2])\n",
    "        start_epoch=int(dt.datetime(start_year, start_month, start_day).timestamp())\n",
    "\n",
    "        submissions=list(api.search_submissions(after=start_epoch,\n",
    "                            subreddit=subreddit_name,\n",
    "                            filter=['url','author', 'title', 'subreddit'],\n",
    "                            limit=sub_limit))\n",
    "\n",
    "    print(\"Harvesting data... This might take a while...\")\n",
    "\n",
    "if measure==\"1\":\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    if \".csv\" in input_filename:\n",
    "        filename=\"hot_submissions_\"+input_filename\n",
    "        filename_2=\"user_aggregates_hot_submissions_\"+input_filename\n",
    "\n",
    "    else:\n",
    "        filename=\"hot_submissions_\"+input_filename+\".csv\"\n",
    "        filename_2=\"user_aggregates_hot_submissions_\"+input_filename+\".csv\"\n",
    "    with open(filename,\"w\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_headers)\n",
    "    with open(filename_2,\"w\", newline='',encoding='utf-8') as q:\n",
    "        wr2 = csv.writer(q, delimiter=\",\")\n",
    "        wr2.writerow(csv_headers_2)\n",
    "    for sub in subreddit.hot(limit=sub_limit):\n",
    "        sub_author=sub.author\n",
    "        sub_downs=sub.downs\n",
    "        sub_ups=sub.ups\n",
    "        sub_likes=sub.likes\n",
    "        sub_title=sub.title\n",
    "        sub_created=sub.created\n",
    "        sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        sub_id=sub.id\n",
    "        sub_reports=sub.num_reports\n",
    "        sub_text=sub.selftext\n",
    "        sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        sub_list=sub.comments.list()\n",
    "        sub_count=sub_count+1\n",
    "        sub_users=[]\n",
    "        sub_users.append(str(sub_author))\n",
    "        csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports, subreddit_name,sub_id]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        for comment in sub_list:\n",
    "            comment_depth=comment.depth\n",
    "            comment_parent_id=comment.parent_id\n",
    "            comment_parent_id=comment_parent_id.split(\"_\")[1]\n",
    "            comment_reports=comment.num_reports\n",
    "            comment_author=str(comment.author)\n",
    "            if comment_author.lower() in blacklisted_users:\n",
    "                continue\n",
    "            comment_body=comment.body\n",
    "            comment_created=comment.created_utc\n",
    "            comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            comment_downs=comment.downs\n",
    "            comment_ups=comment.ups\n",
    "            comment_likes=comment.likes\n",
    "            comment_id=comment.id\n",
    "            sub_users.append(str(comment_author))\n",
    "            comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "            csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports,subreddit_name, sub_id]\n",
    "            with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "                wr = csv.writer(f, delimiter=\",\")\n",
    "                wr.writerow(csv_list)\n",
    "            com_count=com_count+1  \n",
    "        user_entry=\"\"\n",
    "        for user in sub_users:\n",
    "            if not sub_users.index(user)==len(sub_users)-1:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user+\";\"\n",
    "            else:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user\n",
    "        csv_list_2=[str(sub_id), str(sub_url), user_entry,str(subreddit_name)]\n",
    "        with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "            wr2 = csv.writer(q, delimiter=\",\")\n",
    "            wr2.writerow(csv_list_2)\n",
    "        if sub_count % 50 == 0:\n",
    "            print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum \"+str(max_response_cache)+\". Continuing harvest...\")\n",
    "\n",
    "if measure==\"2\":\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    if \".csv\" in input_filename:\n",
    "        filename=\"controversial_submissions_\"+input_filename\n",
    "        filename_2=\"user_aggregates_controversial_submissions_\"+input_filename\n",
    "    else:\n",
    "        filename=\"controversial_submissions_\"+input_filename+\".csv\"\n",
    "        filename_2=\"user_aggregates_controversial_submissions_\"+input_filename+\".csv\"\n",
    "\n",
    "    with open(filename,\"w\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_headers)\n",
    "    with open(filename_2,\"w\", newline='',encoding='utf-8') as q:\n",
    "        wr2 = csv.writer(q, delimiter=\",\")\n",
    "        wr2.writerow(csv_headers_2)\n",
    "    for sub in subreddit.controversial(limit=sub_limit):\n",
    "        sub_comments=sub.num_comments\n",
    "        sub_author=sub.author\n",
    "        sub_downs=sub.downs\n",
    "        sub_ups=sub.ups\n",
    "        sub_likes=sub.likes\n",
    "        sub_title=sub.title\n",
    "        sub_created=sub.created\n",
    "        sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        sub_id=sub.id\n",
    "        sub_reports=sub.num_reports\n",
    "        sub_text=sub.selftext\n",
    "        sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        sub_list=sub.comments.list()\n",
    "        sub_count=sub_count+1\n",
    "        sub_users=[]\n",
    "        sub_users.append(str(sub_author))\n",
    "        csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports,subreddit_name, sub_id]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        for comment in sub_list:\n",
    "            comment_depth=comment.depth\n",
    "            comment_parent_id=comment.parent_id\n",
    "            comment_reports=comment.num_reports\n",
    "            comment_author=str(comment.author)\n",
    "            if comment_author.lower() in blacklisted_users:\n",
    "                continue\n",
    "            comment_body=comment.body\n",
    "            comment_created=comment.created_utc\n",
    "            comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comment_downs=comment.downs\n",
    "            comment_ups=comment.ups\n",
    "            comment_likes=comment.likes\n",
    "            comment_id=comment.id\n",
    "            sub_users.append(str(comment_author))\n",
    "            comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "            csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports,subreddit_name, sub_id]\n",
    "            with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "                wr = csv.writer(f, delimiter=\",\")\n",
    "                wr.writerow(csv_list)\n",
    "            com_count=com_count+1    \n",
    "        user_entry=\"\"\n",
    "        for user in sub_users:\n",
    "            if not sub_users.index(user)==len(sub_users)-1:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user+\";\"\n",
    "            else:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user\n",
    "        csv_list_2=[str(sub_id), str(sub_url), user_entry,str(subreddit_name)]\n",
    "        with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "            wr2 = csv.writer(q, delimiter=\",\")\n",
    "            wr2.writerow(csv_list_2)\n",
    "        if sub_count % 50 == 0:\n",
    "            print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum \"+str(max_response_cache)+\". Continuing harvest...\")\n",
    "\n",
    "if measure==\"3\":\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    if \".csv\" in input_filename:\n",
    "        filename=\"newest_submissions_\"+input_filename\n",
    "        filename_2=\"user_aggregates_newest_submissions_\"+input_filename\n",
    "    else:\n",
    "        filename=\"newest_submissions_\"+input_filename+\".csv\"\n",
    "        filename_2=\"user_aggregates_newest_submissions_\"+input_filename+\".csv\"\n",
    "    with open(filename,\"w\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_headers)\n",
    "    with open(filename_2,\"w\", newline='',encoding='utf-8') as q:\n",
    "        wr2 = csv.writer(q, delimiter=\",\")\n",
    "        wr2.writerow(csv_headers_2)\n",
    "    for sub in subreddit.new(limit=sub_limit):\n",
    "        sub_comments=sub.num_comments\n",
    "        sub_author=sub.author\n",
    "        sub_downs=sub.downs\n",
    "        sub_ups=sub.ups\n",
    "        sub_likes=sub.likes\n",
    "        sub_title=sub.title\n",
    "        sub_created=sub.created\n",
    "        sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        sub_id=sub.id\n",
    "        sub_reports=sub.num_reports\n",
    "        sub_text=sub.selftext\n",
    "        sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        sub_list=sub.comments.list()\n",
    "        sub_count=sub_count+1\n",
    "        sub_users=[]\n",
    "        sub_users.append(str(sub_author))\n",
    "        csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports,subreddit_name, sub_id]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        for comment in sub_list:\n",
    "            comment_depth=comment.depth\n",
    "            comment_parent_id=comment.parent_id\n",
    "            comment_reports=comment.num_reports\n",
    "            comment_author=str(comment.author)\n",
    "            if comment_author.lower() in blacklisted_users:\n",
    "                continue\n",
    "            comment_body=comment.body\n",
    "            comment_created=comment.created_utc\n",
    "            comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comment_downs=comment.downs\n",
    "            comment_ups=comment.ups\n",
    "            comment_likes=comment.likes\n",
    "            comment_id=comment.id\n",
    "            sub_users.append(str(comment_author))\n",
    "            comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "            csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports,subreddit_name, sub_id]\n",
    "            with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "                wr = csv.writer(f, delimiter=\",\")\n",
    "                wr.writerow(csv_list)\n",
    "            com_count=com_count+1\n",
    "        user_entry=\"\"\n",
    "        for user in sub_users:\n",
    "            if not sub_users.index(user)==len(sub_users)-1:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user+\";\"\n",
    "            else:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user\n",
    "        csv_list_2=[str(sub_id), str(sub_url), user_entry,str(subreddit_name)]\n",
    "        with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "            wr2 = csv.writer(q, delimiter=\",\")\n",
    "            wr2.writerow(csv_list_2)\n",
    "        if sub_count % 50 == 0:\n",
    "            print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum \"+str(max_response_cache)+\". Continuing harvest...\")\n",
    "\n",
    "if measure==\"4\":\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    if \".csv\" in input_filename:\n",
    "        filename=\"top_submissions_\"+input_filename\n",
    "        filename_2=\"user_aggregates_top_submissions_\"+input_filename\n",
    "    else:\n",
    "        filename=\"top_submissions_\"+input_filename+\".csv\"\n",
    "        filename_2=\"user_aggregates_top_submissions_\"+input_filename+\".csv\"\n",
    "    with open(filename,\"w\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_headers)\n",
    "    with open(filename_2,\"w\", newline='',encoding='utf-8') as q:\n",
    "        wr2 = csv.writer(q, delimiter=\",\")\n",
    "        wr2.writerow(csv_headers_2)\n",
    "    for sub in subreddit.top(limit=sub_limit):\n",
    "        sub_comments=sub.num_comments\n",
    "        sub_author=sub.author\n",
    "        sub_downs=sub.downs\n",
    "        sub_ups=sub.ups\n",
    "        sub_likes=sub.likes\n",
    "        sub_title=sub.title\n",
    "        sub_created=sub.created\n",
    "        sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        sub_id=sub.id\n",
    "        sub_reports=sub.num_reports\n",
    "        sub_text=sub.selftext\n",
    "        sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        sub_list=sub.comments.list()\n",
    "        sub_count=sub_count+1\n",
    "        sub_users=[]\n",
    "        sub_users.append(str(sub_author))\n",
    "        csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports,subreddit_name, sub_id]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        for comment in sub_list:\n",
    "            comment_depth=comment.depth\n",
    "            comment_parent_id=comment.parent_id\n",
    "            comment_reports=comment.num_reports\n",
    "            comment_author=str(comment.author)\n",
    "            if comment_author.lower() in blacklisted_users:\n",
    "                continue\n",
    "            comment_body=comment.body\n",
    "            comment_created=comment.created_utc\n",
    "            comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comment_downs=comment.downs\n",
    "            comment_ups=comment.ups\n",
    "            comment_likes=comment.likes\n",
    "            comment_id=comment.id\n",
    "            sub_users.append(str(comment_author))\n",
    "            comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "            csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports,subreddit_name, sub_id]\n",
    "            with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "                wr = csv.writer(f, delimiter=\",\")\n",
    "                wr.writerow(csv_list)\n",
    "            com_count=com_count+1\n",
    "        user_entry=\"\"\n",
    "        for user in sub_users:\n",
    "            if not sub_users.index(user)==len(sub_users)-1:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user+\";\"\n",
    "            else:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user\n",
    "        csv_list_2=[str(sub_id), str(sub_url), user_entry,str(subreddit_name)]\n",
    "        with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "            wr2 = csv.writer(q, delimiter=\",\")\n",
    "            wr2.writerow(csv_list_2)\n",
    "        if sub_count % 50 == 0:\n",
    "            print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum \"+str(max_response_cache)+\". Continuing harvest...\")\n",
    "\n",
    "if measure==\"5\":\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    if \".csv\" in input_filename:\n",
    "        filename=\"submissions_\"+start_date+\"_\"+input_filename\n",
    "        filename_2=\"user_aggregates_submissions_\"+start_date+\"_\"+input_filename\n",
    "    else:\n",
    "        filename=\"submissions_\"+start_date+\"_\"+input_filename+\".csv\"\n",
    "        filename_2=\"user_aggregates_submissions_\"+start_date+\"_\"+input_filename+\".csv\"\n",
    "    with open(filename,\"w\", newline='',encoding='utf-8') as f:\n",
    "        wr = csv.writer(f, delimiter=\",\")\n",
    "        wr.writerow(csv_headers)\n",
    "    with open(filename_2,\"w\", newline='',encoding='utf-8') as q:\n",
    "        wr2 = csv.writer(q, delimiter=\",\")\n",
    "        wr2.writerow(csv_headers_2)\n",
    "        \n",
    "    for each in submissions:\n",
    "        sub=reddit.submission(str(each))\n",
    "        sub_author=sub.author\n",
    "        sub_downs=sub.downs\n",
    "        sub_ups=sub.ups\n",
    "        sub_likes=sub.likes\n",
    "        sub_title=sub.title\n",
    "        sub_created=sub.created\n",
    "        sub_created=dt.datetime.utcfromtimestamp(sub_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        subreddit_name=str(sub.subreddit)\n",
    "        sub_id=sub.id\n",
    "        sub_reports=sub.num_reports\n",
    "        sub_text=sub.selftext\n",
    "        sub_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id\n",
    "        sub.comments.replace_more(limit=None)\n",
    "        sub_list=sub.comments.list()\n",
    "        sub_count=sub_count+1\n",
    "        sub_users=[]\n",
    "        sub_users.append(str(sub_author))\n",
    "        csv_list=[\"Submission\",sub_id,sub_author, sub_text,sub_created,sub_ups,sub_downs, sub_likes, \"N/A\", subreddit_name, sub_url, sub_reports,subreddit_name, \"N/A\"]\n",
    "        with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "            wr = csv.writer(f, delimiter=\",\")\n",
    "            wr.writerow(csv_list)\n",
    "        for comment in sub_list:\n",
    "            comment_depth=comment.depth\n",
    "            comment_parent_id=comment.parent_id\n",
    "            comment_parent_id=comment_parent_id.split(\"_\")[1]\n",
    "            comment_reports=comment.num_reports\n",
    "            comment_author=str(comment.author)\n",
    "            if comment_author.lower() in blacklisted_users:\n",
    "                continue\n",
    "            comment_body=comment.body\n",
    "            comment_created=comment.created_utc\n",
    "            comment_created=dt.datetime.utcfromtimestamp(comment_created).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            comment_downs=comment.downs\n",
    "            comment_ups=comment.ups\n",
    "            comment_likes=comment.likes\n",
    "            comment_id=comment.id\n",
    "            sub_users.append(str(comment_author))\n",
    "            comment_url=\"https://www.reddit.com/r/\"+subreddit_name+\"/comments/\"+sub_id+\"/\"+sub_title+\"/\"+comment_id\n",
    "            csv_list=[\"comment\",comment_id, comment_author, comment_body,comment_created, comment_ups,comment_downs,comment_likes,comment_depth,comment_parent_id,comment_url,comment_reports,subreddit_name, sub_id]\n",
    "            with open(filename,\"a\", newline='',encoding='utf-8') as f:\n",
    "                wr = csv.writer(f, delimiter=\",\")\n",
    "                wr.writerow(csv_list)\n",
    "            com_count=com_count+1\n",
    "        \n",
    "        user_entry=\"\"\n",
    "        for user in sub_users:\n",
    "            if not sub_users.index(user)==len(sub_users)-1:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user+\";\"\n",
    "            else:\n",
    "                if user:\n",
    "                    user_entry=user_entry+user\n",
    "        csv_list_2=[str(sub_id), str(sub_url), user_entry,str(subreddit_name)]\n",
    "        with open(filename_2,\"a\", newline='',encoding='utf-8') as q:\n",
    "            wr2 = csv.writer(q, delimiter=\",\")\n",
    "            wr2.writerow(csv_list_2)\n",
    "    \n",
    "        if sub_count % 50 == 0:\n",
    "            print(\"Data harvested from \"+str(sub_count)+\" submissions out of maximum 1000. Continuing harvest...\")\n",
    "\n",
    "print(\"The script is done! We have harvested \"+str(sub_count)+\" submissions with \"+ str(com_count)+\" comments in total.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
